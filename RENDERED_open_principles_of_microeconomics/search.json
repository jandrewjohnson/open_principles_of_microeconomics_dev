[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Big Data for Economists",
    "section": "",
    "text": "Preface\nThis is a a work in progress that combines code from many different aspects of my reserach, including from land-use change modeling (SEALS), ecosystem services modeling (InVEST), general equilibrium modeling (GTAP), APEC 8222 (Big Data for Economists), APEC 8601 (Natural Resource Economics) and several sources.\nThis book makes extensive use of other open-source textbooks. Acknowledgements for source material appear in each relevant file. Most have been modified to fit the topic at hand (unless they are licensed under a No-Derivative Work type of license, in which case they are just provided verbatim for easy access).\nThis will eventually grow to form the core textbook content for new iterations of theses courses as well as the documentation for the respective models."
  },
  {
    "objectID": "Introduction/introduction_big_data.html",
    "href": "Introduction/introduction_big_data.html",
    "title": "1  Earth Economy Modeling",
    "section": "",
    "text": "2 NLCD Zoomed 2"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#overview",
    "href": "Introduction/introduction_big_data.html#overview",
    "title": "1  Earth Economy Modeling",
    "section": "1.1 Overview",
    "text": "1.1 Overview\n\nWhat is Earth-Economy modeling?\nHow does this relate to big data\nExample research topics that use big data\nHow does this connect to econometrics?\nBig data examples from my research\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "Introduction/introduction_big_data.html#to-succeed-in-using-this-book",
    "href": "Introduction/introduction_big_data.html#to-succeed-in-using-this-book",
    "title": "1  Earth Economy Modeling",
    "section": "1.2 To succeed in using this book",
    "text": "1.2 To succeed in using this book\n\nNecessary to succeed: mastering software and code.\n\nApplied Economics itself is shifting in this direction.\n\nMore focus on code expertise.\nDistinguishes us (in a very positive way) from traditional Econ programs.\n\n\nIn this book, we will use both R and Python\n\nR is dominant in applied econometrics (and thus is the basis of our department’s coding approach)\nOther disciplines (including machine learning) use R much less\nIn your career, you will likely have to learn new coding languages.\n\nLet’s become bilingual!\n\n\nI will lead the Python-specific component of the course\n\nWill walk you through installation, language basics etc., and then use it apply machine-learning models to big data"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#your-computer",
    "href": "Introduction/introduction_big_data.html#your-computer",
    "title": "1  Earth Economy Modeling",
    "section": "1.3 Your computer",
    "text": "1.3 Your computer\nThis course will build a modern “scientific computing stack” that is emerging among leading academics and open-source practitioners as an extremly powerful tool. We will work through installation of the programming language and several supporting tools.\nIt is possible to use a PC, Mac or Linux for this course, though all examples will be given on a PC. Becoming skilled in Big Data is partly about mastering the tools and it will be your responsibility to come to class with your computer setup in a way for you to succeed. We will discuss any setup steps necessary in the lecture before it is to be used."
  },
  {
    "objectID": "Introduction/introduction_big_data.html#what-is-big-data",
    "href": "Introduction/introduction_big_data.html#what-is-big-data",
    "title": "1  Earth Economy Modeling",
    "section": "1.4 What is big data?",
    "text": "1.4 What is big data?\n\n1.4.1 Big data means many things to different groups\n\nStandard definition: Data sets that are so large or complex that traditional data processing applications are inadequate\n\nStreams of data (e.g., video collected by a self-driving car)\nMassive consumer data (they are watching you)\nRemotely sensed data (satellites or drones taking pictures of the earth)\nTraditional data, but just really big.\n\nMany related subfields also constitute “what is” big data.\n\nMachine learning (core to this course)\nArtificial Intelligence (AI)\nTechnological advancement in computer science and hardware\nEconometrics exactly as we’ve done before, but just with bigger tables."
  },
  {
    "objectID": "Introduction/introduction_big_data.html#why-should-economists-care-about-big-data",
    "href": "Introduction/introduction_big_data.html#why-should-economists-care-about-big-data",
    "title": "1  Earth Economy Modeling",
    "section": "1.5 Why should economists care about “big data”?",
    "text": "1.5 Why should economists care about “big data”?\nIn the very least, it can create many new sources of useful data.\n\n1.5.1 Voice Analysis\n\nOur input is time-series of amplitude of different pitches\nBut to be useful data, we probably want to know what is the MEANING? Here there are useful methods in Natural Language Processing."
  },
  {
    "objectID": "Introduction/introduction_big_data.html#image-analysis",
    "href": "Introduction/introduction_big_data.html#image-analysis",
    "title": "1  Earth Economy Modeling",
    "section": "1.6 Image Analysis",
    "text": "1.6 Image Analysis\n\nCategorization\n\nEven image generation"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#sentiment-analysis",
    "href": "Introduction/introduction_big_data.html#sentiment-analysis",
    "title": "1  Earth Economy Modeling",
    "section": "1.7 Sentiment Analysis",
    "text": "1.7 Sentiment Analysis"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#big-data-from-remote-sensing-satellites",
    "href": "Introduction/introduction_big_data.html#big-data-from-remote-sensing-satellites",
    "title": "1  Earth Economy Modeling",
    "section": "1.8 Big data from remote sensing (satellites)",
    "text": "1.8 Big data from remote sensing (satellites)\n\nCreates terabytes of information per day\n\nCan assess economic factor like poverty\nOr environmental factors like freshwater availability\n\nData types:\n\nRaster data (matrices of spatial values)\nVector data (link databases of survey data to georeferenced household locations)"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#nlcd",
    "href": "Introduction/introduction_big_data.html#nlcd",
    "title": "1  Earth Economy Modeling",
    "section": "1.9 NLCD",
    "text": "1.9 NLCD"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#nlcd-zoomed",
    "href": "Introduction/introduction_big_data.html#nlcd-zoomed",
    "title": "1  Earth Economy Modeling",
    "section": "1.10 NLCD Zoomed",
    "text": "1.10 NLCD Zoomed"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#nlcd-zoomed-3",
    "href": "Introduction/introduction_big_data.html#nlcd-zoomed-3",
    "title": "1  Earth Economy Modeling",
    "section": "2.1 NLCD Zoomed 3",
    "text": "2.1 NLCD Zoomed 3"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#lots-of-data-same-old-econometrics",
    "href": "Introduction/introduction_big_data.html#lots-of-data-same-old-econometrics",
    "title": "1  Earth Economy Modeling",
    "section": "3.1 1.) Lots of data, same old econometrics",
    "text": "3.1 1.) Lots of data, same old econometrics\nWhen n is very large (or both n and k are)"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#new-prediction-approaches",
    "href": "Introduction/introduction_big_data.html#new-prediction-approaches",
    "title": "1  Earth Economy Modeling",
    "section": "3.2 2.) New prediction approaches",
    "text": "3.2 2.) New prediction approaches"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#so-weve-got-better-models-and-huge-data.-whats-the-risk",
    "href": "Introduction/introduction_big_data.html#so-weve-got-better-models-and-huge-data.-whats-the-risk",
    "title": "1  Earth Economy Modeling",
    "section": "3.3 So we’ve got better models and huge data. What’s the risk?",
    "text": "3.3 So we’ve got better models and huge data. What’s the risk?\n\nRequires rethinking what it means to be ‘good’ at prediction.\n\nIn econometrics, we often measure our prediction quality using in-sample  analysis. For example, with R2.\nNormally we cheer when our p-values are tiny.\nWith big data, our p-values are (almost) ALWAYS tiny.\n\nIs this a good thing?\n\n\nIn the Python component of the course, we’re going to introduce a new metric of prediction quality.\n\nOut-of-sample prediction quality through cross-validation.\nHas been around forever of course, but big data greatly improves opportunities our ability to do cross-validation."
  },
  {
    "objectID": "Introduction/introduction_big_data.html#big-data-improves-opportunities-for-cross-validation",
    "href": "Introduction/introduction_big_data.html#big-data-improves-opportunities-for-cross-validation",
    "title": "1  Earth Economy Modeling",
    "section": "3.4 Big Data improves opportunities for cross-validation",
    "text": "3.4 Big Data improves opportunities for cross-validation\nCross-validation splitting and folding:\n\nWe’ll learn this soon."
  },
  {
    "objectID": "Introduction/introduction_big_data.html#model-complexity",
    "href": "Introduction/introduction_big_data.html#model-complexity",
    "title": "1  Earth Economy Modeling",
    "section": "3.5 Model complexity",
    "text": "3.5 Model complexity\n\nWith big data, you can make your model very, very complex\n\nWhat is the risk of this?\n\nPrediction error out of sample _ _ gets worse"
  },
  {
    "objectID": "Introduction/introduction_big_data.html#another-term-overfitting-vs-underfitting",
    "href": "Introduction/introduction_big_data.html#another-term-overfitting-vs-underfitting",
    "title": "1  Earth Economy Modeling",
    "section": "3.6 Another term: overfitting vs underfitting",
    "text": "3.6 Another term: overfitting vs underfitting\n\nOverfitting a model: making the model overly complex to that accuracy falls on the test data.\n\nWe will talk about ways to methodologically hit the “sweet spot” of model complexity."
  },
  {
    "objectID": "Introduction/introduction_big_data.html#criticism-of-big-data",
    "href": "Introduction/introduction_big_data.html#criticism-of-big-data",
    "title": "1  Earth Economy Modeling",
    "section": "3.7 Criticism of Big Data",
    "text": "3.7 Criticism of Big Data\n\nPredicting the world with big data means we’re focused (obsessed?) by how the world was   in the past   (or at best, present)\n\nEmbeds racism, sexism, etc.\nEnables algorithmic discrimination\n\nBig data: not actually new -- just bigger."
  },
  {
    "objectID": "Python_Introduction/python_setup.html",
    "href": "Python_Introduction/python_setup.html",
    "title": "2  Setup Python",
    "section": "",
    "text": "3 Section 1: Installing Git and getting organized\nhttps://github.com/conda-forge/miniforge#mambaforge\nIf you have an “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option).\nIf you get a “Windows Protected your PC”, click Run Anyway.\nProbably choose Just Me unless you know what you’re doing.\nI strongly recommend you install it in C:\\Users\\&lt;YOUR_USERNAME&gt;\\mambaforge\nKeep all defaults EXCEPT make sure you do check “Add Mambaforge to my PATH environment Variable”\nOn windows, it creates a convenient “Miniforge Prompt”. This launches Window’s “Command Prompt” with Conda activated.\n(You can tell it’s activated because it shows that you’re in the (base) environment.\nOn MacOS, it should already be activated, so just type “mamba init”\ncode.visualstudio.com/download\nI recommend selecting the “User Installer”, 64-bit option for windows. is okay.\nWe’ll talk about it more in class, but VS Code is becoming the most-used “integrated development environment” (IDE) across nearly all programming languages, including remarkable growth among R users.\nApparently that makes me in the tiny minority among hackernews, reddit/r/ProgrammerHumor or other centers of intelligencia for coding\nClick the extensions tab.\nThen install the Python extension\nYou’re done with the setup and can start Python Assignment 0.\nYou will submit into this canvas assignment the output of the code below (this will prove to me you’ve done everything you need to and we’ll be ready to dive into Machine Learning!). If you’re auditing this course, just email me your github name and the output of the assignment below.\nAssignment:\n1. Open a terminal to run Python. If you’re on Windows, you can do this in the start menu by searching for or finding “Miniforge Prompt”. On mac, you can just use the built in terminal.\n2. Enter the following text into the terminal, one line at a time:\nmamba init\nmamba activate 8222env1\npython\nimport hazelbean\nprint(5 + 3)\nprint(hazelbean)\n3. Copy and paste the last line of output from these commands into the Canvas assignment, along with your GitHub username. If everything was done correctly, the outline line should start with “&lt;module ‘hazelbean’”.\n4. Celebrate!\nThe following windows command (replacing the username) will set it to the path.\nSETX PATH “%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\mambaforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\mambaforge;”"
  },
  {
    "objectID": "Python_Introduction/python_setup.html#goals-for-this-video-lecture",
    "href": "Python_Introduction/python_setup.html#goals-for-this-video-lecture",
    "title": "2  Setup Python",
    "section": "2.1 Goals for this video lecture",
    "text": "2.1 Goals for this video lecture\n\n\nInstall Git and integrate with Github\nInstall Visual Studio Code (VS Code)\nInstall Python"
  },
  {
    "objectID": "Python_Introduction/python_setup.html#create-a-new-account-on-github",
    "href": "Python_Introduction/python_setup.html#create-a-new-account-on-github",
    "title": "2  Setup Python",
    "section": "3.1 Create a new account on GitHub",
    "text": "3.1 Create a new account on GitHub\n\n\nGo to github.com\nChoosing your username is a surprisingly important decision.\n\nMost people end up using the same GitHub account across many years/jobs\nChoose your username wisely.\n\nDon’t be xX_noscope_CODR_Xx\n\n\n\n\n\nYou can associate it with whatever email you want, but I would suggest using one you know you have permanent access to.\nE.g., I use my personal gmail account still.\n\nCause I don’t have tenure yet!\nAlso I don’t trust OIT.\n\nPlease don’t tell them I said so.\n\nOf course I don’t trust Google either � so yeah.\n\nPython_Assignment_0 (described in a slide below) will require you to email me your github ID so I can add you to the class repository.\n\n\n\n\n\nAlt text\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "Python_Introduction/jupyter_intro.html#jupyter-has-two-kinds-of-cells",
    "href": "Python_Introduction/jupyter_intro.html#jupyter-has-two-kinds-of-cells",
    "title": "3  Jupyter Intro",
    "section": "3.1 Jupyter has two kinds of cells",
    "text": "3.1 Jupyter has two kinds of cells\n\nMarkdown cells (like this one)\nCode cells (like the next one)\n\nAbove the editor window here, you can click +Code or +Markdown to add new ones.\nAlternatively, right-click on a cell for more options (like splitting a cell into two)\nMarkdown cells are meant for formatted content, pictures, lecture notes etc. and follows the same notation as R-markdown.\nIf you want to edit a markdown cell, double left-click it. To finalize the edits, click the checkmark above (or type ctrl-enter)."
  },
  {
    "objectID": "Python_Introduction/jupyter_intro.html#code-runs-in-the-notebook.",
    "href": "Python_Introduction/jupyter_intro.html#code-runs-in-the-notebook.",
    "title": "3  Jupyter Intro",
    "section": "3.2 Code runs IN the notebook.",
    "text": "3.2 Code runs IN the notebook.\nSelect the python cell below. You can edit it freely. To run it, you can click the triangle button (“play button”) to the upper right of that cell. Alternatively, you can ctrl-enter.\n\na = 5\nb = 4\nsimple_summation = a + b\n\nYou know the cell has run successfully if it gets a green check at the bottom. Also notice that there is a number now in the [ ] box at the bottom left. This indicates which cell this was run in order of all the cells run.\nNotice that it doesn’t output anything, but note that the values are now stored in the Python Kernel (Jupyter Server) and are available to other parts of this notebook.\nIf you want to see a variable outputted, you can just type the variable name.\n\nsimple_summation\n\n9\n\n\nYou can also use the print command, but this will supress non-printed variables.\n\nprint('Rounded: ', simple_summation)\n\nRounded:  9"
  },
  {
    "objectID": "Python_Introduction/jupyter_intro.html#order-matters",
    "href": "Python_Introduction/jupyter_intro.html#order-matters",
    "title": "3  Jupyter Intro",
    "section": "3.3 Order matters",
    "text": "3.3 Order matters\nThe second python cell would fail if the first one wasn’t run.\nYou can run the whole program via the double-play Run icon above."
  },
  {
    "objectID": "Python_Introduction/jupyter_intro.html#in-class-exercise-1.1",
    "href": "Python_Introduction/jupyter_intro.html#in-class-exercise-1.1",
    "title": "3  Jupyter Intro",
    "section": "3.4 In-class exercise 1.1",
    "text": "3.4 In-class exercise 1.1\nBelow, add two cells to this notebook.\nFirst, create a markdown cell where you have a header and some paragraph text. To make something a header in Markdown language, just use a hashtag and a space before the title. To make it a paragraph, just separate it with a blank line in between. Finally, add a bulletted list with a few entries. To do this, just have a dash at the beginning of each new bullet.\nSecond, create a python cell. Save a variable that is the sum of all primes between 3 and 10. Print that sum."
  },
  {
    "objectID": "Python_Introduction/python_assignment_0.html#optional-more",
    "href": "Python_Introduction/python_assignment_0.html#optional-more",
    "title": "4  Exercise 1",
    "section": "4.1 Optional More",
    "text": "4.1 Optional More\nIf you’re looking for more, here is one extra task: Run this notebook in VS Code! We’ll talk more about VS Code in the next class, so don’t worry if this doesn’t make sense. If you do want to run this, open it and then select “Run All” at the top of the editor. It will automatically output how well you did (worth no class credit).\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom osgeo import gdal\nimport hazelbean as hb\n\ndef autograde():\n    \n    if len(str(hb.__name__)) == 9:\n        return '100%'\n    else:\n        return \"0%. Keep trying! Feel free to email me in advance of the first Python lecture. Don't save this for teh last day!\"\n\nprint('Hazelbean library imported successfully:', hb)\n\nprint('Autograding output:', autograde())\n\nHazelbean library imported successfully: &lt;module 'hazelbean' from 'c:\\\\Users\\\\jajohns\\\\AppData\\\\Local\\\\mambaforge\\\\envs\\\\8222env1\\\\lib\\\\site-packages\\\\hazelbean\\\\__init__.py'&gt;\nAutograding output: 100%"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#python-basics",
    "href": "Python_Introduction/python_basics.html#python-basics",
    "title": "5  Python Basics",
    "section": "5.1 Python Basics",
    "text": "5.1 Python Basics\n\n# Comments: The hashtag makes the rest of the line a comment. The more programming you do, the more you focus on making good comments.\n# Jupyter lets you write formatted text, but you'll still want to put comments in the raw python.\n\n# Assign some text (a string) to a variable\nsome_text = 'This is the text.'\n\n# Assign some numbers to variables\na = 5  # Here, we implicitly told python that a is an integer\nb = 4.6  # Here, we told python that b is a floating point number (a decimal)\n\n\nEven though nothing is outputted above, our Python “Kernel” has the values to each variable stored for later use.\n\n\n5.1.1 Important note: Python is not a “typed” language\n\nNotice that above, we added an integer and the float (a floating point number, i.e., one with a decimal point). Python “smartly” redefines variables so that they work together.\nThis is different from other languages which require you to manually manage the “types” of your variables.\n\n\n# Python as a calculator. \nsum_of_two_numbers = a + b\n\n# Printing output to the console\nprint('Our output was', sum_of_two_numbers)\n\nOur output was 9.6\n\n\n\nIn the above, you’ll notice the result was a float.\nIf needed, you can demand that python specify something as a certain type, as below.\n\n\nsum_as_int = int(sum_of_two_numbers)\nsum_as_int_back_to_float = float(sum_as_int)\n\nprint('We lost some precision in this operation:', sum_as_int_back_to_float)\n\nWe lost some precision in this operation: 9.0"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#other-python-types",
    "href": "Python_Introduction/python_basics.html#other-python-types",
    "title": "5  Python Basics",
    "section": "5.2 Other python types",
    "text": "5.2 Other python types\n\n# Reminder, this assumes you have setup an envioronment with conda using:\nlist_1 = [4, 5, 6]\nprint('list_1', list_1)\n\nlist_1 [4, 5, 6]\n\n\n\n# You can embed lists in lists in lists, etc.\nlist_2 = [[5, 3, 5], [6, 6, 5]]\nprint(list_2)\n\n[[5, 3, 5], [6, 6, 5]]\n\n\n\n# Dictionaries\ndictionary_1 = {23: \"Favorite number\", 24: \"Second favorite number\"}\nprint('dictionary_1', dictionary_1)\n\n# Here is a multi line string: (also discusses improved capabilities of an IDE editor)\n\nthings_you_can_do_in_vs_code_that_you_cant_do_without_an_ide = \"\"\"\n1.) Move back and forth in your history of cursor positions (using your mouse forward and back buttons)\n2.) Edit on multiple lines at the same time (hold alt and click new spots)\n3.) Smartly paste DIFFERENT values\n4.) Duplicate lines (ctrl-d)\n5.) Introspection (e.g., jump between function definition and usages)\n6.) Debugging (Interactively walk through your code one line at a time)\n7.) Profiling your code (see which lines take the most time to compute.)\n8.) Keep track of a history of copy-paste items and paste from past copies. (ctrl-shift-v)\n\"\"\"\n\ndictionary_1 {23: 'Favorite number', 24: 'Second favorite number'}"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#looping",
    "href": "Python_Introduction/python_basics.html#looping",
    "title": "5  Python Basics",
    "section": "5.3 Looping",
    "text": "5.3 Looping\n\n\n\nsmall_range = range(0, 10)\nprint('small_range:', small_range)\n\nsmall_range_as_list = list(range(0, 10))\nprint('small_range_as_list:', small_range_as_list)\n\n# Here is a for loop. Also note that python EXPLICITLY USES TAB-LEVEL to denote nested things.\n# I.e., the inner part of the loop is tabbed 1 level up. Python does not use { like  R.\n# I LOVE this notation and it's a big part of why python is so pretty and readable.\nsum = 0 # Set the initial variable values\nnum = 0\nsum_with_some = 0\nfor i in range(100, 136, 3):\n    sum = sum + i\n    num = num + 1\n\n    # loop within a loop\n    for j in range(200, 205):\n        sum_with_some = sum + j\n\nmean = sum / num\nprint('mean', mean)\n\nsmall_range: range(0, 10)\nsmall_range_as_list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nmean 116.5"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#defining-functions",
    "href": "Python_Introduction/python_basics.html#defining-functions",
    "title": "5  Python Basics",
    "section": "5.4 Defining functions",
    "text": "5.4 Defining functions\n\n# Functions\ndef my_function(input_parameter_1, input_parameter_2):\n    product = input_parameter_1 * input_parameter_2\n    return product\n\n# Use the function\nvalue_returned = my_function(2, 7)\nprint(value_returned)\n\n14\n\n\n\n# In-class exercise workspace"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#importing-packages",
    "href": "Python_Introduction/python_basics.html#importing-packages",
    "title": "5  Python Basics",
    "section": "5.5 Importing packages",
    "text": "5.5 Importing packages\n\n\n# Built-in packages via the Python Standard Library\nimport math\nimport os, sys, time, random\n\n# Using imported modules\nnumber_rounded_down = math.floor(sum_of_two_numbers)\nprint(number_rounded_down)\n\n9"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#using-packages-from-elsewhere",
    "href": "Python_Introduction/python_basics.html#using-packages-from-elsewhere",
    "title": "5  Python Basics",
    "section": "5.6 Using packages from elsewhere",
    "text": "5.6 Using packages from elsewhere\nWhen we used Mambaforge, we installed a ton of packages. These were not “built-in” to python like the ones above. Here we will import them into our notebook to use.\nThis will also illustrate the use of numpy. We’ll use it so much we us the as code to name it something shorter.\n\nimport numpy as np # The as just defines a shorter name\n\n# Create an 2 by 3 array of integers\nsmall_array = np.array([[5, 3, 5], [6, 6, 5]])\n\nprint('Here\\'s a small numpy array\\n', small_array)\n\n# Sidenote: from above backspace \\ put in front of a character is the\n# \"escapce character,\" which makes python interpret the next thing as a string or special text operator. \\n makes a line break\n\nHere's a small numpy array\n [[5 3 5]\n [6 6 5]]"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#discussion-point",
    "href": "Python_Introduction/python_basics.html#discussion-point",
    "title": "5  Python Basics",
    "section": "5.7 Discussion point",
    "text": "5.7 Discussion point\nThe array above looks identical to the nested lists we made. It IS NOT! It is a numpy array that is ridiculously fast and can scale up to massive, massive data questions. The optional reading for today (Harris et al. 2020, Nature) discusses how these arrays have formed the backbone of modern scientific computing.\n\nlow = 3\nhigh = 8\nshape = (1000, 1000)\n\nsmallish_random_array = np.random.randint(low, high, shape)\n\nprint('Here\\'s a slightly larger numpy array\\n', smallish_random_array)\n\nHere's a slightly larger numpy array\n [[4 7 3 ... 3 6 6]\n [7 6 4 ... 4 7 7]\n [5 6 3 ... 5 6 3]\n ...\n [4 3 7 ... 4 3 3]\n [7 4 4 ... 3 6 6]\n [4 5 4 ... 4 3 5]]"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#in-class-exercise-2.1",
    "href": "Python_Introduction/python_basics.html#in-class-exercise-2.1",
    "title": "5  Python Basics",
    "section": "5.8 In-class exercise 2.1",
    "text": "5.8 In-class exercise 2.1\nParticipation points note! I will call on a random table to show me their answer via their table’s monitor.\nMake a function that returns the square of a number. Combine the function with a loop to calculate the Sum of Squared Numbers from 1 to 100.\nHINT, ** is the exponent operator in python.\nBONUS: Make sure you’re actually right by inserting a print statement in each step.\nBONUS-bonus: Store each stage of the results in a list using your_list = [] and your_list.append(thing_to_add_to_your_list)"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#arrays-as-objects",
    "href": "Python_Introduction/numpy_arrays.html#arrays-as-objects",
    "title": "6  Numpy Arrays",
    "section": "6.1 Arrays as objects",
    "text": "6.1 Arrays as objects\nThe a variable we defined holds much more information than jsut the raw values. It also gives us useful information necessary for working with really big data.\n\n\n# a is an OBJECT, which has lots of useful attributes, such as:\narray_shape = a.shape\nprint(array_shape)\nprint(a.ndim)\nprint(a.dtype.name)\nprint(a.size)\nprint(a.itemsize) #8 Pro-level question. Why does this return 8? Hint 8 * 8 = 64.\nprint(type(a)) \n\n\n(3, 5)\n2\nint32\n15\n4\n&lt;class 'numpy.ndarray'&gt;"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#creating-an-array-from-values",
    "href": "Python_Introduction/numpy_arrays.html#creating-an-array-from-values",
    "title": "6  Numpy Arrays",
    "section": "6.2 Creating an array from values",
    "text": "6.2 Creating an array from values\n\n\na = np.array([1,2,3,4])  # RIGHT\n# a = np.array(1,2,3,4)    # WRONG: TypeError: array() takes from 1 to 2 positional arguments but 4 were given. Uncomment this to see what happens with error handling.\n\n# 2d version\nb = np.array([(1.5,2,3), (4,5,6)])\n\nprint('b\\n', b)\n\nb\n [[1.5 2.  3. ]\n [4.  5.  6. ]]\n\n\n\n\n# Creating an empty array of zeros # NOTICE the extra paranetheses.\nnp.zeros((3, 4))\n\n# or ones.\nnp.ones((2, 3), dtype=np.int16)                # dtype can also be specified\n\n# or ones.\nr = np.random.random((3, 4))                # dtype can also be specified\n# print('r', r)\n\n# Or even faster, just \"allocate the memory\" with an empty matrix.\nc = np.empty((2,3))"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#array-math",
    "href": "Python_Introduction/numpy_arrays.html#array-math",
    "title": "6  Numpy Arrays",
    "section": "6.3 Array math",
    "text": "6.3 Array math\nNumpy is super smart about doing matrix math across multiple dimensions. Note how in the below, it correctly guesses we wanted to add things element-wise.\n\n# Array math\na = np.array([20, 30, 40, 50.])\nb = np.arange(4)\n\nc = a-b\n\nprint('a', a)\nprint('b', b)\nprint('c', c)\n\na [20. 30. 40. 50.]\nb [0 1 2 3]\nc [20. 29. 38. 47.]\n\n\n\n\n# ** is the exponent operator in python\nd = b**2\nprint('d', d)\n\n# Numpy also has handy array-enabled math operators\ne = 10*np.sin(a)\nprint('e', e)\n\n# Con also create conditional arrays\nf = a&lt;35\nprint('f', f)\n\n\nd [0 1 4 9]\ne [ 9.12945251 -9.88031624  7.4511316  -2.62374854]\nf [ True  True False False]"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#slicing-arrays",
    "href": "Python_Introduction/numpy_arrays.html#slicing-arrays",
    "title": "6  Numpy Arrays",
    "section": "6.4 Slicing Arrays",
    "text": "6.4 Slicing Arrays\nSometimes you want to operate on a subset of an array. Slicing provies a high-performance way of doing this.\n\n\na = np.arange(10)\nb = np.arange(12).reshape(3, 4)\nprint(a)\nprint(b)\n\n# Can access items directly, but need as many indices as there are dimensions\nfirst_value = a[2]\nsecond_value = b[2, 3]\nprint(first_value)\nprint(second_value)\n\n[0 1 2 3 4 5 6 7 8 9]\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n2\n11\n\n\n\n# Can also access \"slices\", which are denoted Start, Stop, Stepsize\nr = a[1: 9: 2]\nprint(r)\n\n[1 3 5 7]\n\n\n\n# If you leave out the number and just have the colon, that means you want to use the default. So below, the\n# ::2 is interpretted as Start:End:Every-Other.\n# 3:: would be Start at the third:End:All.\n# :: would just be all the values.\nr = a[::2]\nprint(r)\n\n[0 2 4 6 8]\n\n\n\n\n# A single colon means also means use the full thing.\nr = a[:]\nprint(r)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\n\n\n# Using slices, you can also set individual elements in the array:\nr[0] = 33\nr[3:5] = 44\nprint('r', r)\n\nr [33  1  2 44 44  5  6  7  8  9]\n\n\n\n\n# Setting in this way also can be done according to a condition:\nr[r &lt;= 6] = 5\nprint('r', r)\n\nr [33  5  5 44 44  5  5  7  8  9]\n\n\n\n\n# Finally, an alternate and possibly more powerful way of setting conditional values is the np.where function\n# This function sets anywhere greater than 10 to be 12, otherwise it keeps it at whatever value was already in r\nr = np.where(r &gt; 10, 12, r)\nprint('r', r)\n\nr [12  5  5 12 12  5  5  7  8  9]\n\n\nFinally, if you want to combine conditionals, when you’re working inside an array you need to use Parentheses, & for and and | for or, as below.\n\nd[(d &gt; 200) & (d &lt; 10000)] = 33"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#lets-talk-about-performance",
    "href": "Python_Introduction/numpy_arrays.html#lets-talk-about-performance",
    "title": "6  Numpy Arrays",
    "section": "6.5 Let’s talk about performance",
    "text": "6.5 Let’s talk about performance\nBig data requires fast algorithms. Let’s introduce a slow way of applying an algorithm, and then the fast way.\n\n\n# Slowly looping over arrays\nfor i in a:\n    r = i**(1/3.)\nprint('r', r)\n\nr 2.080083823051904\n\n\n\n\n# Slowly loop to get the sum of the array\nr = 0\nfor row in b:\n    print('row', row)\n    for value in row:\n        print('value', value)\n        r += value\n\nprint('slow sum', r)\n\nrow [0 1 2 3]\nvalue 0\nvalue 1\nvalue 2\nvalue 3\nrow [4 5 6 7]\nvalue 4\nvalue 5\nvalue 6\nvalue 7\nrow [ 8  9 10 11]\nvalue 8\nvalue 9\nvalue 10\nvalue 11\nslow sum 66\n\n\nNOTE: Iterating over arrays here is just for illustration as it is VERY VERY SLOW and loses the magic of numpy speed. We’ll learn how to bet around this later by “vectorizing” functions, which basically means batch calculating everything in a vector all in one call. For now, here’s an example of the much faster version\n\n\nr = b.sum()\nprint('fast sum', r)\n\nfast sum 66"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#diving-into-vectorized-computation.",
    "href": "Python_Introduction/numpy_arrays.html#diving-into-vectorized-computation.",
    "title": "6  Numpy Arrays",
    "section": "6.6 Diving into vectorized computation.",
    "text": "6.6 Diving into vectorized computation.\nHere we are going to do matrix math, but using the fast numpy methods.\n\n\n# Vectorized multiplication (and broadcasting):\n\n# First lets make two arrays. This is the cannonical way of making example arrays\na = np.arange(20).reshape(5, 4)\nb = np.arange(20).reshape(5, 4)\nprint(a)\nprint(b)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]]\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]]\n\n\n\nc = a * b # NOTE: this does element-wise multiplication, not the matrix multiplication you learned in 7-th? grade.\nprint(c)\n\n[[  0   1   4   9]\n [ 16  25  36  49]\n [ 64  81 100 121]\n [144 169 196 225]\n [256 289 324 361]]"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#numpy-broadcasting",
    "href": "Python_Introduction/numpy_arrays.html#numpy-broadcasting",
    "title": "6  Numpy Arrays",
    "section": "6.7 Numpy broadcasting",
    "text": "6.7 Numpy broadcasting\nNumpy will smartly “broadcast” two matrices of different sizes or dimensions so that it works:\n\nd = np.arange(4)\ne = a * d # WAIT! Aren't you multiplying two different matrices with different sizes? Yes!\nprint(e)\n\n[[ 0  1  4  9]\n [ 0  5 12 21]\n [ 0  9 20 33]\n [ 0 13 28 45]\n [ 0 17 36 57]]\n\n\nAbove, Numpy smartly figured out how the two dimensions could be repeatedly broadcast to each other so the math was “well defined.”\n\n\n# Also means you can use the same notation to multiply an array (2dim) against a scalar (0dim):\nf = a * 6.0\nprint('f\\n', f)\n\nf\n [[  0.   6.  12.  18.]\n [ 24.  30.  36.  42.]\n [ 48.  54.  60.  66.]\n [ 72.  78.  84.  90.]\n [ 96. 102. 108. 114.]]\n\n\nLet’s plot the results of this new f matrix we created.\n\n\nax = plt.imshow(b)\nplt.show()\n\n\n\n\nLooks very similar to above, so let’s add a colorbar to make clear this value is not just our starting array.\n\nax = plt.imshow(b)\nplt.colorbar(ax)\nplt.show()"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#plot-mandlebrot",
    "href": "Python_Introduction/numpy_arrays.html#plot-mandlebrot",
    "title": "6  Numpy Arrays",
    "section": "6.8 Plot Mandlebrot",
    "text": "6.8 Plot Mandlebrot\nWith these tools, we can do all sorts of things. Just for fun, let’s end this section by defining a function for the Mandlebrot set and then plotting the function.\n\n\ndef mandelbrot(h, w, maxit=20 ):\n    \"\"\"Returns an image of the Mandelbrot fractal of size (h,w).\"\"\"\n    y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ]\n    c = x+y*1j\n    z = c\n    divtime = maxit + np.zeros(z.shape, dtype=int)\n\n    for i in range(maxit):\n        z = z**2 + c\n        diverge = z*np.conj(z) &gt; 2**2            # who is diverging\n        div_now = diverge & (divtime==maxit)  # who is diverging now\n        divtime[div_now] = i                  # note when\n        z[diverge] = 2                        # avoid diverging too much\n\n    return divtime\n\n\nplt.imshow(mandelbrot(400, 400))\nplt.show()"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#in-class-exercise-3.1",
    "href": "Python_Introduction/numpy_arrays.html#in-class-exercise-3.1",
    "title": "6  Numpy Arrays",
    "section": "6.9 In-class exercise 3.1:",
    "text": "6.9 In-class exercise 3.1:\nAgain, I’ll call on a random table to showcase their results.\n\nCreate a 20 by 40 matrix of random values 0-1 (Hint: use the np.random.random function. Use VS Code’s built-in help to see what you should enteR).\nSet the upper left quadrant to 1. (Hint: use slices)\nNext, set the last COLUMN to 2.\nFinally, change all values less than .5 to be 3. (Use np.where)\n\n\n# Class activity workspace"
  },
  {
    "objectID": "Python_Introduction/pandas_tables.html",
    "href": "Python_Introduction/pandas_tables.html",
    "title": "7  Pandas Tables",
    "section": "",
    "text": "import pandas\nimport os\n\ndata_directory = '../data'\nfood_prices_filename = 'world_monthly_food_prices.csv'\nfood_prices_path = os.path.join(data_directory, food_prices_filename)\nfood_prices = pandas.read_csv(food_prices_path)\n\nprint('Whole dataframe:', food_prices)\n\nWhole dataframe:                                      Domain Code                  Domain  \\\n0                                             CP  Consumer Price Indices   \n1                                             CP  Consumer Price Indices   \n2                                             CP  Consumer Price Indices   \n3                                             CP  Consumer Price Indices   \n4                                             CP  Consumer Price Indices   \n..                                           ...                     ...   \n176                                           CP  Consumer Price Indices   \n177                                           CP  Consumer Price Indices   \n178                                           CP  Consumer Price Indices   \n179                                           CP  Consumer Price Indices   \n180  FAOSTAT Date: Wed Aug 17 16:12:56 CEST 2016                     NaN   \n\n     AreaCode AreaName  ElementCode ElementName  ItemCode  \\\n0      5000.0    World       7001.0     January   23013.0   \n1      5000.0    World       7001.0     January   23013.0   \n2      5000.0    World       7001.0     January   23013.0   \n3      5000.0    World       7001.0     January   23013.0   \n4      5000.0    World       7001.0     January   23013.0   \n..        ...      ...          ...         ...       ...   \n176    5000.0    World       7012.0    December   23013.0   \n177    5000.0    World       7012.0    December   23013.0   \n178    5000.0    World       7012.0    December   23013.0   \n179    5000.0    World       7012.0    December   23013.0   \n180       NaN      NaN          NaN         NaN       NaN   \n\n                                       ItemName    Year  Value  Flag  \\\n0    Consumer Prices, Food Indices (2000 = 100)  2000.0   99.5   NaN   \n1    Consumer Prices, Food Indices (2000 = 100)  2001.0  101.5   NaN   \n2    Consumer Prices, Food Indices (2000 = 100)  2002.0  106.5   NaN   \n3    Consumer Prices, Food Indices (2000 = 100)  2003.0  112.5   NaN   \n4    Consumer Prices, Food Indices (2000 = 100)  2004.0  119.2   NaN   \n..                                          ...     ...    ...   ...   \n176  Consumer Prices, Food Indices (2000 = 100)  2011.0  210.3   NaN   \n177  Consumer Prices, Food Indices (2000 = 100)  2012.0  224.2   NaN   \n178  Consumer Prices, Food Indices (2000 = 100)  2013.0  239.0   NaN   \n179  Consumer Prices, Food Indices (2000 = 100)  2014.0  250.9   NaN   \n180                                         NaN     NaN    NaN   NaN   \n\n             FlagD  \n0    Official data  \n1    Official data  \n2    Official data  \n3    Official data  \n4    Official data  \n..             ...  \n176  Official data  \n177  Official data  \n178  Official data  \n179  Official data  \n180            NaN  \n\n[181 rows x 12 columns]\n\n\n\nprint('List of column names:', food_prices.columns)\n\nList of column names: Index(['Domain Code', 'Domain', 'AreaCode', 'AreaName', 'ElementCode',\n       'ElementName', 'ItemCode', 'ItemName', 'Year', 'Value', 'Flag',\n       'FlagD'],\n      dtype='object')\n\n\n\nprint('Specific column:', food_prices['Value'])\n\nSpecific column: 0       99.5\n1      101.5\n2      106.5\n3      112.5\n4      119.2\n       ...  \n176    210.3\n177    224.2\n178    239.0\n179    250.9\n180      NaN\nName: Value, Length: 181, dtype: float64\n\n\n\nprint('Specific value in that column:', food_prices['Value'][6])\n\nSpecific value in that column: 132.2\n\n\n\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\nplt.plot(food_prices['Value'])\nplt.show() \n\n\n\n\n\n\nimport os\n\nimport numpy as np\nimport pandas as pd\n\n# Set a seed value for the random generator\nnp.random.seed(48151623)\n\n# Creating a Series by passing a list of values, letting pandas create a default integer index:\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\n\n# Pandas is very detailed in dealing with dates and all the quirks (leap year?) that this leads to.\ndates = pd.date_range('20130101', periods=6)\n\n# Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns:\ndf = pd.DataFrame(np.random.randn(6, 4), columns=list('ABCD'))\nprint('df:\\n', df)\n\n\ndf2 = pd.DataFrame({'A': 1.,\n                    'B': pd.Timestamp('20130102'),\n                    'C': pd.Series(1, index=list(range(4)), dtype='float32'),\n                    'D': np.array([3] * 4, dtype='int32'),\n                    'E': pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n                    'F': 'foo'})\n\n# df.head()\n# print(df.index)\n# print(df.columns)\ndf.describe()\n\n# Also note that a dataframe is really just a numpy array dressed up with extra trappings. If you want you\n# can get back the raw array (though this might lose a lot of functionality).\na = df.to_numpy()\nprint('a\\n', a)\n\n# Sorting Values:\n\n# Also, I want to illustrate THE MOST COMMON MISTAKE people make with Pandas.\n\n# The sort_values method (a method is just a function attached to an object) returns a NEW modified dataframe.\n# Thus, in the line below, if you just printed df, it would not be sorted because we didn't use the returned value.\ndf.sort_values(by='B')\n# print('Not sorted:\\n', df)\n\n# Easy way to get around this is just to assign the returned dataframe to a variable (even the input variable)\ndf = df.sort_values(by='B')\n# print('Sorted with return:\\n', df)\n\n# Alternatively, if you hate returning things, there is the inplace=True command, which will modify the df ... inplace.\ndf.sort_values(by='B', inplace=True)\n# print('Sorted inplace:\\n', df)\n\n## Selection/subsetting of data\n\n# Selecting a single column, which yields a Series, equivalent to df.A\ndf['A']\ndf.A\n\n# Selecting via [], which slices the rows.\ndf[0:3] # CAN BE SLOW\n\n# Note, slicing above, which uses the\n# standard Python / Numpy expressions for selecting and setting are intuitiveits best to use\n# the optimized pandas data access methods, .at, .iat, .loc and .iloc.\n\n## Selecting by LABELS, loc and iloc\n\nr = df.loc[0] # 0-th row.\n\n# print('r', r)\n\n# Discuss difference between df['A'] and df.loc[0]\nr = df.loc[0, 'A']\n\nr = df.loc[:, 'A'] # Colon is a slice, an empty colon means ALL the values.\n\n# OPTIMIZATION:\n# for faster single point access, use:\nr = df.at[0, 'A']\n\n# SELECTING BY POSITION\nr = df.iloc[3]\n\n# Selecting with slices\nr = df.iloc[3:5, 0:2]\n\n# Slices again with an empty slice.\nr = df.iloc[1:3, :]\n\nr = df.iloc[:, 1:3]\n\n# SIMILAR OPTIMIZATION:\nr = df.iat[1, 1]\n\n# Boolean indexing\n# Using a single column’s values to select data.\nr = df[df['A'] &gt; 0]\n\n# Make a copy (why?) and add a column\ndf2 = df.copy()\ndf2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']\nr = df2[df2['E'].isin(['two', 'four'])]\n\n\n# Setting by assigning with a NumPy array:\ndf.loc[:, 'D'] = np.array([5] * len(df))\n\n# Missing data\n\n# First we're going to create a new df by \"reindexing\" the old one, which will shuffle the data into a new\n# order according to the index provided. At the same time, we're going to add on a new, empty column\n# EE, which we set as 1 for the first two obs.\n\ndf1 = df.reindex(index=[2, 0, 1, 3], columns=list(df.columns) + ['EE'])\ndf1.loc[0:1, 'EE'] = 1\n# print(df1)\n\n# Apply: Similar to R. Applies a function across many cells (fast because it's vectorized)\ndf.apply(np.cumsum)\ndf.apply(lambda x: x.max() - x.min())\n\n# Concat\ns = pd.Series(range(0, 6))\n# print('s', s)\n\nr = pd.concat([df, s]) # Concatenate it, default is by row, which just puts it on the bottom.\n\nr = pd.concat([df, s], axis=1) # Concatenate as a new column\n\n# print(r) # Result when concatenating a series of the same size.\n\ns = pd.Series(range(0, 7))\nr = pd.concat([df, s], axis=1) # Concatenate as a new column\n\ns = pd.Series(range(0, 2))\nr = pd.concat([df, s], axis=1) # Concatenate as a new column\n\n# Join\n# SQL style merges. See the Database style joining section.\n\nleft = pd.DataFrame({'key': ['foo', 'bar'], 'lval': [1, 2]})\nright = pd.DataFrame({'key': ['foo', 'bar'], 'rval': [4, 5]})\n\n# print(left)\n# print(right)\n\ndf = pd.merge(left, right, on='key')\n\n# print('df:\\n', df)\n\n# Stacking\nstacked = df.stack()\n# print('stacked:\\n', stacked)\n\n\n# Pivot Tables\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three'] * 3,\n                   'B': ['A', 'B', 'C'] * 4,\n                   'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2,\n                   'D': np.random.randn(12),\n                   'E': np.random.randn(12)})\n\n# print(df) # SPREADSHEET VIEW\ndf = pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])\n# print(df) # Multiindexed (Pivot table) view.\n\n# NOTICE that a pivot table is just the above date but where specific things have been made into multi-level\n# indices.\n\n# PLOTTING\nts = pd.Series(np.random.randn(1000),\n            index=pd.date_range('1/1/2000', periods=1000))\n\nts = ts.cumsum()\nts.plot()\nimport matplotlib.pyplot as plt\n# plt.show()\n\n\n# Writing to files\n\ndf.to_csv('foo.csv')\n\n# Reading files:\n\n# FIRST NOTE, here we are using relative paths (which you should almost always do too). the ../ means go up one level.\n# this path works if you organized your data into the folder structure I suggested.\nwdi_filename = \"WDI_CO2_data.csv\"\nwdi_path = os.path.join(data_directory, wdi_filename)\ndf = pd.read_csv(wdi_path)\n\nprint('csv read as a df\\n', df)\n\n# For reference, here's the Excel version\n# df = pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])\n\ncols = list(df.columns)\n\n# Make a subset of only 2 cols\nr = df[['Country Code', '1970 [YR1970]']]\n# print(r)\n\nr = df.loc[df['Country Code'] == 'CAN']\n# print('r', r)\n\nrr = r.loc[df['Series Name'] == 'Total greenhouse gas emissions (kt of CO2 equivalent)']\nprint(rr)\n\n# Class exercise: Plot the emissions of CO2 for Canada (or whereever I don't care).\n\ndf:\n           A         B         C         D\n0  0.336581  0.923754 -0.277124  0.388604\n1 -1.295428  3.296657 -0.698246  0.245552\n2 -1.086536  1.187113 -0.153344 -1.264476\n3  0.798694  1.330577 -0.113975 -0.060949\n4 -0.076321 -0.240310  0.728421 -0.384309\n5  0.634212  1.605129  1.415844  0.385849\na\n [[ 0.33658127  0.92375415 -0.27712413  0.38860406]\n [-1.2954285   3.29665716 -0.69824576  0.24555238]\n [-1.0865361   1.18711302 -0.15334368 -1.26447611]\n [ 0.79869365  1.33057707 -0.11397528 -0.06094946]\n [-0.07632075 -0.24031028  0.72842085 -0.38430931]\n [ 0.6342119   1.60512881  1.41584369  0.38584939]]\ncsv read as a df\n                                           Country Name Country Code  \\\n0                                          Afghanistan          AFG   \n1                                          Afghanistan          AFG   \n2                                          Afghanistan          AFG   \n3                                          Afghanistan          AFG   \n4                                          Afghanistan          AFG   \n...                                                ...          ...   \n2640                                               NaN          NaN   \n2641                                               NaN          NaN   \n2642                                               NaN          NaN   \n2643  Data from database: World Development Indicators          NaN   \n2644                          Last Updated: 10/15/2020          NaN   \n\n                                            Series Name           Series Code  \\\n0     Access to clean fuels and technologies for coo...        EG.CFT.ACCS.ZS   \n1               Access to electricity (% of population)        EG.ELC.ACCS.ZS   \n2     Adjusted net enrollment rate, primary (% of pr...           SE.PRM.TENR   \n3                          Arable land (% of land area)        AG.LND.ARBL.ZS   \n4     Agricultural methane emissions (thousand metri...  EN.ATM.METH.AG.KT.CE   \n...                                                 ...                   ...   \n2640                                                NaN                   NaN   \n2641                                                NaN                   NaN   \n2642                                                NaN                   NaN   \n2643                                                NaN                   NaN   \n2644                                                NaN                   NaN   \n\n     1960 [YR1960]     1961 [YR1961]     1962 [YR1962]     1963 [YR1963]  \\\n0               ..                ..                ..                ..   \n1               ..                ..                ..                ..   \n2               ..                ..                ..                ..   \n3               ..  11.7176730079956  11.7942591060871  11.8708452041785   \n4               ..                ..                ..                ..   \n...            ...               ...               ...               ...   \n2640           NaN               NaN               NaN               NaN   \n2641           NaN               NaN               NaN               NaN   \n2642           NaN               NaN               NaN               NaN   \n2643           NaN               NaN               NaN               NaN   \n2644           NaN               NaN               NaN               NaN   \n\n       1964 [YR1964]   1965 [YR1965]  ...     2011 [YR2011]     2012 [YR2012]  \\\n0                 ..              ..  ...             22.33             24.08   \n1                 ..              ..  ...  43.2220189082037              69.1   \n2                 ..              ..  ...                ..                ..   \n3     11.94743130227  11.94743130227  ...  11.9336458046135  11.9321140826517   \n4                 ..              ..  ...                ..                ..   \n...              ...             ...  ...               ...               ...   \n2640             NaN             NaN  ...               NaN               NaN   \n2641             NaN             NaN  ...               NaN               NaN   \n2642             NaN             NaN  ...               NaN               NaN   \n2643             NaN             NaN  ...               NaN               NaN   \n2644             NaN             NaN  ...               NaN               NaN   \n\n         2013 [YR2013]    2014 [YR2014]    2015 [YR2015]     2016 [YR2016]  \\\n0                26.17            27.99             30.1             32.44   \n1     68.9332656860352             89.5             71.5              97.7   \n2                   ..               ..               ..                ..   \n3     11.9244554728426  11.903011365377  11.893821033606  11.8386790429801   \n4                   ..               ..               ..                ..   \n...                ...              ...              ...               ...   \n2640               NaN              NaN              NaN               NaN   \n2641               NaN              NaN              NaN               NaN   \n2642               NaN              NaN              NaN               NaN   \n2643               NaN              NaN              NaN               NaN   \n2644               NaN              NaN              NaN               NaN   \n\n     2017 [YR2017]     2018 [YR2018] 2019 [YR2019] 2020 [YR2020]  \n0               ..                ..            ..            ..  \n1             97.7  98.7132034301758            ..            ..  \n2               ..                ..            ..            ..  \n3               ..                ..            ..            ..  \n4               ..                ..            ..            ..  \n...            ...               ...           ...           ...  \n2640           NaN               NaN           NaN           NaN  \n2641           NaN               NaN           NaN           NaN  \n2642           NaN               NaN           NaN           NaN  \n2643           NaN               NaN           NaN           NaN  \n2644           NaN               NaN           NaN           NaN  \n\n[2645 rows x 65 columns]\n    Country Name Country Code  \\\n369       Canada          CAN   \n\n                                           Series Name        Series Code  \\\n369  Total greenhouse gas emissions (kt of CO2 equi...  EN.ATM.GHGT.KT.CE   \n\n    1960 [YR1960] 1961 [YR1961] 1962 [YR1962] 1963 [YR1963] 1964 [YR1964]  \\\n369            ..            ..            ..            ..            ..   \n\n    1965 [YR1965]  ...     2011 [YR2011]     2012 [YR2012] 2013 [YR2013]  \\\n369            ..  ...  1033481.98200961  1027063.85487082            ..   \n\n    2014 [YR2014] 2015 [YR2015] 2016 [YR2016] 2017 [YR2017] 2018 [YR2018]  \\\n369            ..            ..            ..            ..            ..   \n\n    2019 [YR2019] 2020 [YR2020]  \n369            ..            ..  \n\n[1 rows x 65 columns]\n\n\nC:\\Users\\jajohns\\AppData\\Local\\Temp\\ipykernel_26860\\2184278581.py:107: FutureWarning:\n\nIn a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html",
    "href": "Python_Introduction/gdal_rasters.html",
    "title": "8  GDAL Rasters",
    "section": "",
    "text": "9 Saving a raster to your harddrive\nNow that you’ve created an amazing raster of total maize production, you might want to save it to your harddrive.\nTo do this, we’re first going to define a new filename for our output file. In the code below, + concatenates things. Str() makes the number a string.\noutput_filename = 'gdal_created_array_' + str(random.randint(1, 1000000)) + '.tif'\noutput_file_path = os.path.join(data_directory, output_filename)\nCreate a new file at that filename location using the attributes we used above. Notice that we flipped n_cols and n_rows from how numpy would have wanted it. For extra BONUS value, replace the d array with the one you created in the in-class exercise.\noutput_dataset = gdal.GetDriverByName('GTiff').Create(output_file_path, d.shape[1], d.shape[0], 1, 6)\nSet dataset-level information. Here we’re just using what we got from the input raster, defined above.\noutput_dataset.SetGeoTransform(geotransform)\noutput_dataset.SetProjection(projection)\n\n0\nNow get a band from our new dataset on which we’ll write our array.\noutput_band = output_dataset.GetRasterBand(1)\nDo the array writing\noutput_band.WriteArray(d)\n\n0\nSet any final band-level information\noutput_band.SetNoDataValue(no_data_value)\n\n0\nFinally, and very importantly, clean up after yourself. It wont actually write until the resources in memory have been released.\nd = None\noutput_band = None\noutput_dataset = None"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#spatial-data-in-jupyter",
    "href": "Python_Introduction/gdal_rasters.html#spatial-data-in-jupyter",
    "title": "8  GDAL Rasters",
    "section": "8.1 Spatial data in Jupyter",
    "text": "8.1 Spatial data in Jupyter\nFirst we’re going to import gdal, numpy and a few other things.\n\nfrom osgeo import gdal\nimport numpy as np\nimport os, random\n\nFirst, define the path to our raster data. This is remarkably difficult (at least, as measured by how many hours I have wasted because my code pointed to the wrong place). A the superior way to manage this is with RELATIVE PATHS. Here, we define the filename, the directory in relative terms, and then join them together using the os functions.\n\ngeotiff_filename = 'rwanda_lulc_2015.tif'\n\n# the ../ notation means go up one level relative to your current working directory. This gets us outside of the course\n# repository and into our Data directory\ndata_directory = '../data' \n\n# Join them together (this will work across operating systems)\ngeotiff_file_path = os.path.join(data_directory, geotiff_filename)\n\n\n\nIt’s often easy to get confused by relative paths, accidentally missing a level or something. One way to trouble-shoot this is to view the current working direcotry:\n\nprint(os.getcwd())\n\nD:\\My Drive\\Files\\Teaching\\jupyter_based_courses\\earth_economy_textbook\\earth_economy_textbook_dev\\01_Python_Introduction\n\n\nOr to view the absolute path:\n\nos.path.abspath(data_directory)\n\n'D:\\\\My Drive\\\\Files\\\\Teaching\\\\jupyter_based_courses\\\\earth_economy_textbook\\\\earth_economy_textbook_dev\\\\data'\n\n\nSometimes it can be useful to see what is in the directory you’ve specified (to help you figure out what’s going on if it can’t find the file)\n\ncontents = os.listdir(data_directory)\nprint(contents)\n\n['rwanda_lulc_2015.tif', 'world_monthly_food_prices.csv', 'rwanda_lulc_2000.tif', 'rwanda_lulc_2010.tif', 'WDI_CO2_data.csv', 'gdal_created_array_737809.tif', 'gdal_created_array_828327.tif']\n\n\n\n# If you want to be super clear, you can actually check if it exists\nprint(os.path.exists(geotiff_file_path))\n\nTrue"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#gdal-and-opening-the-raster",
    "href": "Python_Introduction/gdal_rasters.html#gdal-and-opening-the-raster",
    "title": "8  GDAL Rasters",
    "section": "8.2 GDAL and opening the raster",
    "text": "8.2 GDAL and opening the raster\nNow that we know for sure that the file is there, we can use gdal and it’s Open function using the DOT notation (technically its a “method” not a function, but you can ignore that. If you’re actually curious about object-oriented programming, a method is just a function attached to an object.).\n\nmaize_production_tons_per_cell = gdal.Open(geotiff_file_path)\nprint(maize_production_tons_per_cell)\n\n&lt;osgeo.gdal.Dataset; proxy of &lt;Swig Object of type 'GDALDatasetShadow *' at 0x000001D92F0A8030&gt; &gt;\n\n\nThe dataset object holds information about the area and extent of the data, or the geotransform information\n\ngeotransform = maize_production_tons_per_cell.GetGeoTransform()\nprojection = maize_production_tons_per_cell.GetProjection()\n\nprint('GDAL dataset geotransform', geotransform)\nprint('GDAL dataset projection', projection)\n\nGDAL dataset geotransform (28.855555555555558, 0.002777777777777778, 0.0, -1.0583333333333333, 0.0, -0.002777777777777778)\nGDAL dataset projection GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n\n\nIMPORTANT ANNOYING NOTE: in programming, there are different conventions for identifying a place by rows, cols vs. x, y vs. upper-left, lower-right, etc. Numpy is denoted row, col but gdal is denoted X, Y (which flips the order). Just memorize that row = Y and col = X.\n\nn_rows = maize_production_tons_per_cell.RasterYSize\nprint('Number of rows in a GDAL dataset', n_rows)\n\nn_cols = maize_production_tons_per_cell.RasterXSize\nprint('Number of columns in a GDAL dataset', n_cols)\n\nNumber of rows in a GDAL dataset 637\nNumber of columns in a GDAL dataset 732\n\n\nNext, get the “band” of the dataset. Many datasets have multiple layers (e.g. NetCDFs). Geotiffs can have multiple bands but often have just 1. For now, grab band 1\n\nmaize_production_tons_per_cell_band = maize_production_tons_per_cell.GetRasterBand(1)\n\nThe band object has information too, like the datatype of the geotiff:\n\ndata_type = maize_production_tons_per_cell_band.DataType\nno_data_value = maize_production_tons_per_cell_band.GetNoDataValue()\n\nprint('data_type', data_type)\nprint('no_data_value', no_data_value)\n\ndata_type 1\nno_data_value 255.0\n\n\nFinally, we can get the array from the band as a numpy array:\n\narray = maize_production_tons_per_cell_band.ReadAsArray()\nshape = array.shape\n\nprint('Look at the array itself', array)\n\nLook at the array itself [[255 255 255 ... 255 255 255]\n [255 255 255 ... 255 255 255]\n [255 255 255 ... 255 255 255]\n ...\n [255 255 255 ... 255 255 255]\n [255 255 255 ... 255 255 255]\n [255 255 255 ... 255 255 255]]"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#plotting-a-raster",
    "href": "Python_Introduction/gdal_rasters.html#plotting-a-raster",
    "title": "8  GDAL Rasters",
    "section": "8.3 Plotting a raster",
    "text": "8.3 Plotting a raster\nWe are now going to use matplotlib. It is basically like ggplot and draws its inspiration from MATLAB notation. By convention, we’ll import it into the variable name plt, which is an object that lets us use matplotlib plotting notation.\n\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(array)\nplt.title('Land-use, land-cover')\nplt.colorbar(orientation='horizontal')\n\n# Uncomment this if you want to save it\n# plt.savefig('maize.png', dpi=300) \n\nplt.show()\n\n\n\n\nThis is super ugly for one primary reason: it has scaled the colorbar to the minimum and maximum values, which ends up coloring nearly everything close to the zero value. We’re going to crop the values it shoes to not let the outliers define the colorbar range.\n\nimport matplotlib.pyplot as plt\n\n# The second (and often better) plotting method is to use plt to create a figure and one ore more axes.\n# This is potentially confusing but is powerful. the Axes object we created (ax) is the plottable area (and there could be lots of axes)\n# The figure contains all the axes and is responsible for organizing stuff.\n\nfig = plt.figure(figsize=(8, 6))\nfig.set_dpi(300)\n\nax = fig.add_subplot()\n\n# Set the title of this ax object\nax.set_title('Land-use, land-cover')\n\n# Using the ax we created, we call the imshow function on our array from earlier. This create a new \"im\" object\nim = ax.imshow(array)\n\n# To fix the outlier problem from before, we use the im object to set its limits.\nim.set_clim(0, 200)\n\n# Add the colorbar to the figure. It will generate its values from the im object.\nfig.colorbar(im, orientation='horizontal')\n\nplt.show()"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#copies-versus-views-in-numpy",
    "href": "Python_Introduction/gdal_rasters.html#copies-versus-views-in-numpy",
    "title": "8  GDAL Rasters",
    "section": "8.4 Copies versus Views in Numpy",
    "text": "8.4 Copies versus Views in Numpy\nPart of how Numpy arrays are fast is the only ever load or access data when it is needed. This means that if you don’t tell numpy to make a copy of something, any new variable will point to the old array. More specifically, this only creates a new pointer to the same block of memory on your computer that holds the array. If we change c_view, c will also be changed. So in the below, c_view only points to the old data in c. This is called a “view” of the array.\n\nc_view = array\n\nThis also means that if you modify array, you will be modifying what you have in c_view.\nIf you really need a copy in memory, you can use the numpy method copy():\n\nd = array.copy()\n\nThis gives us a NEW array in a new block of memory, so changing array will not change d,"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#in-class-exercise-4.1",
    "href": "Python_Introduction/gdal_rasters.html#in-class-exercise-4.1",
    "title": "8  GDAL Rasters",
    "section": "8.5 In-class exercise 4.1",
    "text": "8.5 In-class exercise 4.1\nHere you will use GDAL and numpy to do some highly-optimized raster manipulation.\nUsing data from Earthstat, I want you to calculate the production per grid-cell of Maize, globally, at “high”-resolution.\nThe two files you need are in your class data directory (obtained from google drive). On my computer they are saved in the following locations.\nD:\\My Drive\\Files\\Teaching\\APEC 8222\\Data\\maize_HarvestedAreaHectares.tif\nD:\\My Drive\\Files\\Teaching\\APEC 8222\\Data\\maize_YieldPerHectare.tif\nUse the os.path.join() approach from earlier to correcly make relative paths (NOT ABSOLUTE PATHS LIKE I PASTED ABOVE) pointing to the two files.\nOpen them up as raster using Gdal.\nMultiply the HarvestedAreaHectars by YieldPerHectare. This will give you the total production on the grid-cell.\nUse numpy to sum up the total production of Maize globally and report that to the class."
  },
  {
    "objectID": "Python_Introduction/python_assignment_1.html#question-1",
    "href": "Python_Introduction/python_assignment_1.html#question-1",
    "title": "9  Exercise 2",
    "section": "9.1 Question 1",
    "text": "9.1 Question 1\nThis question tests some basic python comprehension.\nThroughout, use comments to explain steps you are doing. No need to go overboard but it is always important in coding to be as descriptive as possible.\nMost things in this can be solved using methods shown in lecture. However, some cases will require you to search the internet for answers. This is intended because efficiently searching documentation or Stackoverflow is a requirement of modern programing.\nSubsequent questions will be specific to big data, statistics and economics, but for now we are just checking that you’re okay with the basics of Python.\nPart A: Write a python function that calculates the sum of all squared numbers from 1 to x. Illustrate that it works for x = 20.\nHINT, ** is the exponent operator in python. HINT syntax for a python function is:\ndef function_name(variable_name): outcome = variable_name + 1 return outcome\n\n# 1A Answer\n\nThe python library named “os” is a built-in library for dealing with any file on your operating system. Often, research tasks involve LOTS of files and you need to iterate over them. To show you know how to do this, use the os.listdir function to answer the following questions. Note that you need to write “import os” to import the library into your code before you can use it:\nPart B: Print out a list of all the files in the class repository (which you have gotten from GitHub). I don’t care how many are actually there (in case you’ve added some yourself) but show me how.\n\n#1B Answer\n\nPart C: Using a FOR loop, iterate over the list from above. Using the function len(), count how many letters there are in the filenames. HINT just like in real life, you may need to google the len() function and see how it work on e.g. strings.\n\n# 1C Answer\n\nPart D: Write a Python program which iterates over the integers from 1 to 50. For multiples of three print “Fizz” instead of the number and for the multiples of five print “Buzz”. For numbers which are multiples of both three and five print “FizzBuzz”. Hint: look up how to use the modulo operator (which is written as % in python) which would let you test when the remainder of a devision is exactly 0.\nSide-note, this is a hilariously over-used question that most software engineers get askedon their first interview for a job.\n\n# 1D Answer"
  },
  {
    "objectID": "Python_Introduction/python_assignment_1.html#question-2",
    "href": "Python_Introduction/python_assignment_1.html#question-2",
    "title": "9  Exercise 2",
    "section": "9.2 Question 2",
    "text": "9.2 Question 2\nPart a.) Using the gdal package and the gdal.Open() function, open up the land-use, land-cover map for Rwanda in 2000.\nIn this file, there is only 1 band in this file, so you can also access it with the GetRasterBand(1) function. Without reading the whole array, show how you can determine how many total grid-cells there are in this country.\n\n# 2A Answer\n\nPart b.) Using the results of part a, read the whole array into memory as a numpy array (the default option when using the ReadAsArray() funciton, and plot it using the matplotlib imshow command. Add a nice title to the plot describing what it is.\n\n#2B Answer\n\nPart c.) Using the legend you find at https://maps.elie.ucl.ac.be/CCI/viewer/download/ESACCI-LC-QuickUserGuide-LC-Maps_v2-0-7.pdf reclassify the LULC into a simplified map where 1 = cropland (including any mosaic types that are partially cropland) and 0 = anything else. Plot this using imshow.\n\n# 2C Answer\n\nPart d.) Repeat the process for the 2010 LULC map. Using this array with the one from part c, create a new array that records where there was cropland expansion (i.e., there is cropland in 2010 but not in 2000) and where there was cropland abandonment (cropland in 2000 but not in 2010). Save this classification in a single new array. Plot this last array. Optionally, add a legend indicating which values in the array denote expansion and abandonment using some variant of ax.legend()\n\n# 2D Answer\n\nPart e.) Overall, did Rwanada see net expansion or contraction of cropland over this period? Looking at the spatial results, do you see any patterns of where this happens? Is it in the north/south, near cities, replacing forests etc.?\n\n#2E Answer"
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html",
    "href": "Machine_Learning/introduction_big_data.html",
    "title": "10  Machine Learning Intro",
    "section": "",
    "text": "11 Cross-Validation, Python for Linear Regression, and Support Vector Machines\nReminder as always: Pull the latest code (I probably made some minor updates)\nRelated fun OSS product of the day: QUARTO!\n\n\n12 Syllabus check-in\n\nI’ve also added some optional journal articles to Canvas that exemplify the approaches we’re using\n\n\n13 Agenda\nFinal word on Git: pushing and pulling.\nDiscuss cross-validation (CV) vs. model performance\nIntroduce Scikit-learn via regular linear regression, use it to talk about CV\nIntroduce Support Vector Machines for classification\n\n\n14 Git push and pull\n\n\n15 What happens when I make a change and how does everyone stay synced.\n\n\n16 Suppose I make an edit on my Workstation\nSuppose I ad some extremely important new code to our workbook from last lecture.\n\n\n\n17 Git realizes that my file now doesn’t match the repository\nFirst thing you’ll notice is the file changes to brown/orange and has an “M” for modified by it.\n\n\n18 How do I get this into the online repository?\n\nClick on the Source Control tab and you will see this file listed in the “Change List”\nBecause I’m in charge of this repository, I want to “commit” this file and then “push” it to the repository.\n\nI type a commit message (REQUIRED and will silently fail if not), and then select Commit and Push.\n\nNow my Source Control tab is clean. That means my local code matches the remote repository.\n\n\n\n\n\n19 But wait, what happens to other people who also have edited the file?\nGitting into a predicament.\n\n\n20 Suppose your instructor says “okay, now pull the latest code from the course repository�”\n\nIf you do that, VS Code might scold you.\nWhat does this mean?\nIt means YOU have changes on your computer that are different from what’s on GitHub.\n\nGit can’t pull because it doesn’t know how to resolve the conflict.\n\n\n\n\n\n21 Source control tab shows new code is available.\nGo to the Source Control tab\nIf your files are different than the repository, you will see those files listed here.\n\n\n22 How do we resolve this “merge conflict”?\n\nIf you do not want to keep your changes (simplest case) we can discard them and then pull.\nTo do this, go to the Source Control tab and you will see the Change List.\nRight-click, and select discard changes.\nNow you can happily Git Pull.\n\n\n\n23 You can also keep both sets of changes by “merging” them\n\nHere’s an example where I might want to keep them.\nI clicked on the change list and opened it. You can see where I modified the In-class exercise.\nYou can research more about this on your own, but for now we’re just going to avoid it.\n\n\n24 I think the best way to do this is to just move this to your own folder outside the repository.\n\nThere you can preserve all your notes.\nOnce you moved the file (not copied), Git Pull will succeed at getting the newest course code.\n\n\n25 Cross-validation\nIn my opinion, this is the most important advance in all of Machine Learning from the perspective of an applied economist.\n\n\n26 Cross-validation vs model performance\n\nIn either econometrics or ML, there are two tasks in building a model\n\nEstimating parameters of the model\nEvaluating how well that model does\n\nDifferent approaches for these steps between Econometrics and ML:\n\nEconometrics the above steps involve:\n\nt- and p-statistics, hypothesis testing, analyzing specific coefficients\nR-values, AIC/BIC, etc\n\nIn ML, the emphasis is different. The above steps in ML are:\n\nMostly absent in isolation, but included in step 2.\nDetermined by cross-validation of the model.\n\n\n\n\n\n27 Returning to the complexity tradeoff.\n\nRecall: Overfitting a model is making the model overly complex to that accuracy falls on the test data.\n\nWe will talk about ways to methodologically hit the “sweet spot” of model complexity.\n\nHow do we find this sweet-spot? Cross validation\nFirst though, let’s illustrate why overly-complex models can UNDER-perform.\n\n\nThe source of the “sweet spot” in model complexity.\n\nThe complex model is super accurate on the training data.\n\nWith new data, the complex model is much worse. Notice that the simple model performs about the same.\nImage source: statquest.org\n\n\n28 Operationalizing Cross-Validation\n\n\n29 Splitting data and Cross-Validation\n\n\nIn this course, we will use scikit-learn to illustrate this.\nScikit-learn has nice built-in functions to split our data into training and test data.\n\nThis is the first step of cross-validation approaches.\n\nWe are going to set aside the data and make sure we never use it again until the very end.\n\n\n\n30 We train the model on a second split of the data\n\n\nTo train our model, cross-validation creates a second split of the training data.\n\nML algorithms will iteratively try different models/coefficients on this second spit, using whichever performs best on the training-test data.\nBut we can do MORE than that!\n\n\n\n\n\n31 Splitting into MANY Splits and Folds\n\n\n\n32 Final model performance analysis\n\n\nOnce the best set of parameters are found, the model is compared against the test data from the first split.\nFinal performance assessment then is done with calculating the MSE of the model prediction of Test_X for Test_Y\nThis method is SUPER FLEXIBLE\n\nIt could compare totally dissimilar models in a rigorous way.\nWill helps us choose “tuning-” or “hyper-parameters”\n\n\n\n\n33 Switch to VS Code\nOpen Lectures\\02_Machine_Learning\\01_Linear_Regression.ipynb\n\n\n34 Support Vector Machines\n\n\n35 Support vector machines\nMotivation: classify inputs into categories\nApproach: draw a line (hyperplane) separating observations from different categories\nIssue 1: Many lines might work. Which one should we choose?\nIssue 1: Many lines might work. Which one should we choose?\nA: Pick the line with the largest ‘margin’ � distance to nearest points on either side\n\n\n36 SVMs: naming digression\nThose nearest points determine the ‘support vectors’. I think of it as a little person heroically holding up the margin lines.\n\n\n\n\n37 SVM: math preliminaries\n\n\n\n38 SVM: objective\n\n\n39 Support vector machines\nBut, maybe no lines work perfectly. What can we do?\n\n\n40 SVM: objective\nwhere add some sort of penalty on misclassifications.\nMany forms of this penalty term are possible. Here is the simplest one that just says limit sum of misclassification to be below some threshold gamma.\nYou might be wondering, wouldn’t this all depend on the gamma value?\nYes it does. And we will use Cross-validation to find the best value for this “hyperparameter”.\n\n\n41 We choose gamma via CV!\n\n\nIteratively try all values of gamma.\nWhichever one predicts best across the many splits is what we will use.\nThus, we have systematically determined exactly how many outliers we should ignore\n\nFrom the perspective of out-of-sample performance.\n\n\n\n\n42 Switch to VS Code\nOpen Lectures\\02_Machine_Learning\\02_SVM.ipynb"
  },
  {
    "objectID": "Machine_Learning/linear_regression.html#exercise-1",
    "href": "Machine_Learning/linear_regression.html#exercise-1",
    "title": "11  Linear Regression",
    "section": "11.1 Exercise 1:",
    "text": "11.1 Exercise 1:\nPrint out the mean BMI in the dataset, the sum BMI, and the sum of the squared BMI values. Explain why the sum of the squared BMI is what it is. To do this, you will need to access the right parts of the data array and slice out the right column.\nHINT: You will need to read the DESCR to understand which column the BMI is stored in.\nHINT 2: To create a new variable with just the desired column of the array, you can use Array slicing notation like a = data_array[:, n] where the : means you want ALL rows, and the n means you want just column n.\nHINT 3: You may want to use data_array.sum(), data_array.mean(), and the ** exponent operator.\n\nbmi = data_array[:, 2]\n\nbmi_squared = bmi ** 2\n\nprint('Sum: ', bmi.mean())\nprint('Mean: ', bmi.mean())\nprint('Sum of squared: ', bmi_squared.mean())\n\nSum:  -2.2455642172282577e-16\nMean:  -2.2455642172282577e-16\nSum of squared:  0.0022624434389140265\n\n\n\n# For conveinence, sklearn also just has an option to get the key parts for the regression ready to use.\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n\n\n# Look at diabetes_X and notice there are lots of independent variables. Rather than printing the whole\n# Array, which would be messy, just look at the .shape attribute.l\n# print('diabetes_X', diabetes_X)\nprint('diabetes_X', diabetes_X.shape)\n\ndiabetes_X (442, 10)\n\n\nFor now, we’re just going to use a single variable (a single column) for simplicity. The following line extracts just the second column from the array. The colon was necessary because we access arrays using the ROW, COLUMN notation, so we sliced out all ROWS (the colon indicates all) and the second COLUMN.\n\ndiabetes_X = diabetes_X[:, 2]\n# diabetes_X = np.array([diabetes_X])\n# diabetes_X = diabetes_X[:, np.newaxis, 2]\n# print('diabetes_X', diabetes_X)\nprint('diabetes_X', diabetes_X.shape)\n\n# diabetes_X = diabetes_X.reshape((:, 1))\ndiabetes_X = diabetes_X.reshape((diabetes_X.shape[0], 1))\n\n# print('diabetes_X', diabetes_X)\nprint('diabetes_X', diabetes_X.shape)\n\ndiabetes_X (442,)\ndiabetes_X (442, 1)\n\n\n\n11.1.1 Split into training and testing arrays (the manual way)\nNext we are going to do a very rudimentary split of the data into training and testing sets using array slice notation. The following lines assigns the last all but the last 20 lines to the TRAIN set and the remaining 20 to the test set.\n\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\nCreate an empty LinearRegression object.\nIn the lines below, we will follow a relatively standardized process for running a model:\n\nCreate the model object.\nFit the model.\nPredict with the model\n\nThe basic notation for sklearn below first creates a regression model object using the linear_model that we imported above. This model is “empty” in the sense that it has no coefficients identified. Just like othertimes we’ve encountered objects (like numpy array objects), this object has many functions (called methods) and attributes which can be accessed by the dot operator.\n\nregression_object = linear_model.LinearRegression()\nprint('regression_object', regression_object)\n\nregression_object LinearRegression()\n\n\n\n11.1.1.1 Use the fit method\nUse the fit method from our regression object. It takes two inputs, the independent variables (X) and dependent variables (y).\nBelow, we will ONLY use the training subset of the data we created above.\n\nregression_object.fit(diabetes_X_train, diabetes_y_train)\nprint(regression_object)\n\nLinearRegression()\n\n\n\n\n11.1.1.2 Use the fitted model to predict values\nNow the regression_object is “trained,” which means we can also call it’s predict() method which will take some other observations and (in the case of OLS), multiple the new observations against our trained coefficients to make a prediciton.\nThe predict method returned an array of numerical predictions, which we will look at.\n\ndiabetes_y_pred = regression_object.predict(diabetes_X_test)\nprint(diabetes_y_pred)\n\n[225.9732401  115.74763374 163.27610621 114.73638965 120.80385422\n 158.21988574 236.08568105 121.81509832  99.56772822 123.83758651\n 204.73711411  96.53399594 154.17490936 130.91629517  83.3878227\n 171.36605897 137.99500384 137.99500384 189.56845268  84.3990668 ]\n\n\n\n\n11.1.1.3 Look at the coefficients\nMore interesting might be to look at the coefficients. Once the model has been fit, it has a new attribute .coef_ which stores an array of coefficients. In this case it will only be an array of length 1 because we just have one input.\n\nprint('Coefficients: \\n', regression_object.coef_)\n\nCoefficients: \n [938.23786125]\n\n\nYou might be wondering why we are looking at the coefficients as a raw array rather than at a nicely formatted regression table. The reason is in cross-validation approaches, these coefficients might just be one step towards the final model performance check on unseen data.\n\n\n11.1.1.4 Evaluating the fit\nWe can use sklearn’s built in evaluation functions, such as for the mean squared error or other metrics.\n\nmse = mean_squared_error(diabetes_y_test, diabetes_y_pred)\nprint('Mean squared error on the TEST data:',  mse)\n\nMean squared error on the TEST data: 2548.07239872597\n\n\n\n\n# Or perhaps we want the r2 for the second independent variable (which is the only one we used)\nr2_score_value = r2_score(diabetes_y_test, diabetes_y_pred)\nprint('r2 calculated on TEST data: ', r2_score_value)\n\nr2 calculated on TEST data:  0.47257544798227147\n\n\n\n# Finally, to prove to ourselves that we know what we are doing, let's plot this.\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/linear_regression.html#exercise-2.1-machine-learning-ols-mashup.",
    "href": "Machine_Learning/linear_regression.html#exercise-2.1-machine-learning-ols-mashup.",
    "title": "11  Linear Regression",
    "section": "11.2 Exercise 2.1: Machine Learning OLS Mashup.",
    "text": "11.2 Exercise 2.1: Machine Learning OLS Mashup.\nUse loops to find which TWO variables best describe the data, as measured by R-squared. This is a hilariously brute-force approach to OLS model selection, but it is similar in some senses to Machine Learning and will be relevant to the cross-validation approaches we discuss next.\n\n# Exercise 2.1 workspace and starter code\n\n\nfull_dataset = datasets.load_diabetes() # Load the full dataset\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True) # Get just the data arrays\n\n# Split into training and testing\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\nhighest_score = 0\nfor i in range(len(full_dataset['feature_names'])):\n    for j in range(len(full_dataset['feature_names'])):\n        \n        diabetes_current_X_train = diabetes_X_train[:, [i, j]]\n        diabetes_current_X_test = diabetes_X_test[:, [i, j]]\n\n        # MISSING STUFF HERE.       \n        \n        if r2_score_value &gt; highest_score:\n            highest_score = r2_score_value\n            best_option = [i, j, r2_score_value]\n        \nprint('best_option', best_option)\n        \n        \n\nbest_option [0, 0, 0.47257544798227147]"
  },
  {
    "objectID": "Machine_Learning/linear_regression.html#just-for-completeness-lets-look-at-this-the-way-an-econometritian-would",
    "href": "Machine_Learning/linear_regression.html#just-for-completeness-lets-look-at-this-the-way-an-econometritian-would",
    "title": "11  Linear Regression",
    "section": "11.3 Just for completeness, let’s look at this the way an econometritian would",
    "text": "11.3 Just for completeness, let’s look at this the way an econometritian would\nSklearn doesn’t report summary statistics in the classic, econometric sense because it focuses on the train, test paradigm, which is not equivilent to a model performance report (which in the classic case is only reporting performance of the TRAINING data).\nNonetheless, Here’s how I do it, using an alternative, more econometrics-focused package. You will need to conda install statsmodel if you want to uncomment this line and have it work. Note that because we’re not splitting our data into training and testing, the r-squareds are not really comparable.\n\nimport statsmodels\nfrom statsmodels.api import OLS\n\ndata_with_constant = statsmodels.api.add_constant(full_dataset.data)\nresult = OLS(full_dataset.target, data_with_constant).fit().summary()\nprint(result)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.518\nModel:                            OLS   Adj. R-squared:                  0.507\nMethod:                 Least Squares   F-statistic:                     46.27\nDate:                Thu, 29 Dec 2022   Prob (F-statistic):           3.83e-62\nTime:                        14:01:42   Log-Likelihood:                -2386.0\nNo. Observations:                 442   AIC:                             4794.\nDf Residuals:                     431   BIC:                             4839.\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.1335      2.576     59.061      0.000     147.071     157.196\nx1           -10.0099     59.749     -0.168      0.867    -127.446     107.426\nx2          -239.8156     61.222     -3.917      0.000    -360.147    -119.484\nx3           519.8459     66.533      7.813      0.000     389.076     650.616\nx4           324.3846     65.422      4.958      0.000     195.799     452.970\nx5          -792.1756    416.680     -1.901      0.058   -1611.153      26.802\nx6           476.7390    339.030      1.406      0.160    -189.620    1143.098\nx7           101.0433    212.531      0.475      0.635    -316.684     518.770\nx8           177.0632    161.476      1.097      0.273    -140.315     494.441\nx9           751.2737    171.900      4.370      0.000     413.407    1089.140\nx10           67.6267     65.984      1.025      0.306     -62.064     197.318\n==============================================================================\nOmnibus:                        1.506   Durbin-Watson:                   2.029\nProb(Omnibus):                  0.471   Jarque-Bera (JB):                1.404\nSkew:                           0.017   Prob(JB):                        0.496\nKurtosis:                       2.726   Cond. No.                         227.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "Machine_Learning/support_vector_machines.html",
    "href": "Machine_Learning/support_vector_machines.html",
    "title": "12  Support Vector Machines",
    "section": "",
    "text": "In this section, we will use what we learned about fitting models and apply it to a very useful machine-learning algorithm.\nFirst let’s start with imports.\n\nimport numpy as np\nimport scipy\nimport sklearn\nfrom sklearn import datasets\nimport pandas as pd\nimport os\n\n\n12.0.0.1 Load in some digit image data\nOne of the canonical datasets in sklearn is a series of images of handwritten digits. We’ve imported the datasets above, but now lets load it.\n\n\ndigits = datasets.load_digits()\n\n# First, take a look at the raw python object:\nprint('digits\\n', digits)\n\ndigits\n {'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n       ...,\n       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n       [ 0.,  0., 10., ..., 12.,  1.,  0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'frame': None, 'feature_names': ['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7'], 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n        ...,\n        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n\n       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n\n       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n        ...,\n        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n\n       ...,\n\n       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n        ...,\n        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n\n       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n        ...,\n        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n\n       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n        ...,\n        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]), 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}\n\n\nNot super helpful unless you’re very good at reading python dictionary notation. Fortunately, one of the entries in this dataset is a description. Let’s read that.\n\n\nprint('DESCR\\n', digits['DESCR'])\n\nDESCR\n .. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 1797\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n\n\n\n\n\n12.0.0.2 Extract one of the digits to inspect\nNow that we’re oriented, also look at one particular image of a digit, just so you know what it actually looks like. Below, we print just the first (index = 0) numeral of the 5620 they provide.\n\n\nprint('digits.images[0]\\n', digits.images[0])\n\ndigits.images[0]\n [[ 0.  0.  5. 13.  9.  1.  0.  0.]\n [ 0.  0. 13. 15. 10. 15.  5.  0.]\n [ 0.  3. 15.  2.  0. 11.  8.  0.]\n [ 0.  4. 12.  0.  0.  8.  8.  0.]\n [ 0.  5.  8.  0.  0.  9.  8.  0.]\n [ 0.  4. 11.  0.  1. 12.  7.  0.]\n [ 0.  2. 14.  5. 10. 12.  0.  0.]\n [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n\n\n\n\n# If you squint, maybe you can tel what image it is, but let's plot it to be sure.\nimport matplotlib\nfrom matplotlib import pyplot as plt\nplt.imshow(digits.images[0])\nplt.show()\n\n\n\n\nNotice also in the dataset that there is a ‘targets’ attribute in the dataset. This is the correct numeral that we are trying to make the model predict.\n\nprint('target', digits.target)\n\ntarget [0 1 2 ... 8 9 8]\n\n\nOur task now is to train a model that inputs the digit images and predicts the digit numeral. For this, we’re going to use SVM, as discussed in lecture.\n\n\n12.0.0.3 Import SVM and create a new (unfitted) model with it.\nFor now, the parameters are going to be manually set (gamme) but we’ll address how to choose them later. Here, I want to illustrate the basic approach used in sklearn to Load, train, fit and predict the model\n\nfrom sklearn import svm\n\n# Create the model object\nclassifier = svm.SVC(gamma=0.001)\n\nAt this point, classifier is not yet “trained”, ie. not yet fit to the model. All ML algorithms in SKLEARN have a .fit() method, which we will use here, passing it the images and the targets.\nBefore we train it, we want to split the data into testing and training splits.\nClass question: Remind me WHY are we splitting it here? What is the bad thing that would happen if we just trained it on all of them?\nBefore we can even split the data, however, we need to reshape it to be in the way the regression model expects.\nIn particular, the SVM model needs a 1-dimensional, 64 element array. BUT, the input digits we saw were 2-dimensional, 8 by 8 arrays.\nThis actually leads to a somewhat mind-blown example of how computers “think” differently than we do. We clearly think about a numeral in 2 dimensional space, but here we see that the computer doesn’t are about the spatial relationship ship at all. It sees each individual pixel as it’s own “Feature” to use the classification parlance. You could even reshuffle the order of those 64 digits and as long as you kept it consistent across the data, it would result in identical predictions.\nLater on, we will talk about machine learning techniques that leverage rather than ignore this 2 dimensional, spatial nature of the data.\nFor now, let’s just look at the data again. Rather than print it out, I really just want the shape so that i don’t get inundated with text.\n\nprint('digits.images shape', digits.images.shape)\n\ndigits.images shape (1797, 8, 8)\n\n\n\n\nn_samples = len(digits.images)\nn_features = digits.images[0].size\n\nprint('n_samples', n_samples)\nprint('n_features', n_features)\n\ndata = digits.images.reshape((n_samples, n_features))\n\n# Now check the shame again to see that it's right.\nprint('data shape', data.shape)\n\nn_samples 1797\nn_features 64\ndata shape (1797, 64)\n\n\n\n\n# Now that we've arranged our data in this shape, we can split it into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, shuffle=False)\n\nprint('X_train', X_train)\nprint('y_train', y_train)\n\nX_train [[ 0.  0.  5. ...  0.  0.  0.]\n [ 0.  0.  0. ... 10.  0.  0.]\n [ 0.  0.  0. ... 16.  9.  0.]\n ...\n [ 0.  0.  2. ... 14.  0.  0.]\n [ 0.  1. 12. ...  0.  0.  0.]\n [ 0.  0.  0. ...  3.  0.  0.]]\ny_train [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0\n 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9\n 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4\n 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7\n 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2\n 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 3 1 3 9 1\n 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 5 4 8 8 4 9 0 8 9 8 0 1 2\n 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9\n 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8\n 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2\n 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0\n 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2\n 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7\n 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1\n 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8\n 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2\n 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7\n 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9\n 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1\n 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1\n 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0\n 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9\n 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5\n 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4\n 7 2 8 2 2 5 7 9 5 4]\n\n\n\n\n12.0.0.4 Fit the model\nFinally, now that we’ve split it, we can call the classifier’s fit method which takes the TRAINING data as input.\n\nclassifier.fit(X_train, y_train)\n\nSVC(gamma=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma=0.001)\n\n\nNow, our classifier object has it’s internal parameters fit so that when we give it new input, it predicts what it thinks the correct classification is.\n\npredicted = classifier.predict(X_test)\n\n# Looking at the predicted won't be very intuitive, but you could glance.\nprint('predicted', predicted)\n\npredicted [8 8 4 9 0 8 9 8 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 9 6 7 8 9\n 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 9 1 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9\n 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1\n 7 5 4 4 7 2 8 2 2 5 7 9 5 4 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\n 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 7 8 2 0\n 1 2 6 3 3 7 3 3 4 6 6 6 9 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 5 4 6 3 1 7 9\n 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8\n 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0\n 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9\n 5 2 8 2 0 0 1 7 6 3 2 2 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 8 7 5 4\n 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 9 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7\n 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2\n 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 6 2 8 3 0 0 1 7 6 3 2 1 7 4 6 3 1 3\n 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 0\n 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9\n 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5\n 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4\n 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 1 7 8 9 0 1 2 3 4 5 6 9 0\n 1 2 3 4 5 6 7 8 9 4 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2\n 6 8 7 7 7 3 4 6 6 6 9 9 1 5 0 9 5 2 8 0 1 7 6 3 2 1 7 9 6 3 1 3 9 1 7 6 8\n 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 2 5 7 3 5 9 4 5 0 8 9 8 0 1 2 3 4 5 6 7\n 8 9 0 1 2 8 4 5 6 7 8 9 0 1 2 5 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7\n 7 5 1 0 0 2 2 7 8 2 0 1 2 6 8 8 7 5 8 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7\n 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 5 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7\n 9 5 4 8 8 4 9 0 8 9 8]\n\n\n\n\n12.0.0.5 Plot some results\nLet’s plot a few of them in nicer format. Don’t worry about learning the plotting code but it’s a useful example to show the power.\n\n\n_, axes = plt.subplots(2, 4)\nimages_and_labels = list(zip(digits.images, digits.target))\nfor ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Training: %i' % label)\n\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\nfor ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Prediction: %i' % prediction)\nplt.show()\n\n\n\n\n\n\nfrom sklearn import metrics\n\nprint(\"Classification report:\\n\", metrics.classification_report(y_test, predicted))\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99        88\n           1       0.99      0.97      0.98        91\n           2       0.99      0.99      0.99        86\n           3       0.98      0.87      0.92        91\n           4       0.99      0.96      0.97        92\n           5       0.95      0.97      0.96        91\n           6       0.99      0.99      0.99        91\n           7       0.96      0.99      0.97        89\n           8       0.94      1.00      0.97        88\n           9       0.93      0.98      0.95        92\n\n    accuracy                           0.97       899\n   macro avg       0.97      0.97      0.97       899\nweighted avg       0.97      0.97      0.97       899\n\n\n\n\n\n12.0.0.6 Confusion matrix\nA more convenient way of looking at the results is t the confusion matrix. This is a built in metric for sklearn. It plots the predicted labels vs. the true labels.\n\ndisp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\ndisp.figure_.suptitle(\"Confusion Matrix\")\n\n# print(\"Confusion matrix:\\n\", disp.confusion_matrix)\n\n# Finally, show it so that you can look at it and see how good we did.\nplt.show()\n\nC:\\Users\\jajohns\\AppData\\Local\\mambaforge\\envs\\8222env1\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n\nFunction plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n\n\n\n\n\n\nQUESTION: Which digit was hardest to categorize and what was it most frequently confused as?"
  },
  {
    "objectID": "Machine_Learning/regularization_intro.html",
    "href": "Machine_Learning/regularization_intro.html",
    "title": "13  Regularization Intro",
    "section": "",
    "text": "14 Remember to pull the latest code!\nReminder: HW4 due by midnight tonight.\n\n\nFun trick of the day: You can use Markdown in Google Docs! (have to enable it first)\n\n\n15 Agenda\n\nComment on our hilariously “effective” p-hacking routine from last lecture.\n\nWe’ll address how can we do this in a legitimate way.\n\nIntroduce regularization and shrinkage\n\nDiscuss why we should do it.\n\nLearn Ridge and LASSO ML algorithms\nApply them in Code\n\n\n\n16 Econometrics versus CV\n\nEconometrics: X, Y not split into testing and training.\n\nY_hat is generated from OLS operating on All Data.\n\nIn the linear_regression notebook, we used Sklearn to automatically solve for the best set of (two) coefficients.\n\nIn this, we took our first baby step to CV by splitting into Training and Testing.\nWe learned SVM applied to this training/testing paradigm.\n\nThis is still not “Real CV” because we used the Test Data in finding the best set of coefficients.\n\n\n\n\n17 Real CV\n\n\nTherefor, cross-validation creates a second split of the training data.\n\nIteratively solves on this second-split.\n\n\n\n\n\n18 But that process can be extrapolated up to better utilize our (possibly scarce) data\n\n\nKeeping track of splits and folds is hard\n\nWe’re going to use GridSearchCV to do it all for us.\n\n\n\n\n19 Reminder on the bias-variance tradeoff\n\nReminder: ML lingo uses the word “Variance” in a slightly different way than we’re used to:\n\nHere, variance measures the difference in performance between on the training vs testing data\n\nThis is almost unrelated to the more familiar (to us) use of variance to describe a distribution.\n\n\n\n\n\n\n\n20 One way to reduce variance: Shrinkage\n\nShrinkage estimators offer lower variance by construction\n\nIt’s literally just lowering your beta values.\nThe more we shrink coefficient estimates, the lower their variance across samples.\n\nTo build intuition on this, start with the extreme case: shrink all slope estimates to zero, make constant predictions.\n\nPerformance would be constant!\n\n\n\n\n21 Quick example and implications for bias\n\n\n22 Starting with OLS\n\nWhen we have many observations, OLS does a good job\n\nWhy? Because with tons of observations, it is more likely the training data reflect other possible data.\n\nE.g., suppose we have measurements of weight and size of mice. Fits pretty well.\n\n\n\n\n\n\n23 But what if we have fewer data?\n\nSuppose we only get to fit the data on a small subset (red)\n\nThe resulting estimate is not similar to the full data in green\n\n\n\n\n\n24 Variance on out-of-sample observations is huge\n\n\nOur predicted line will have minimum bias on the training data\n\nIt’s the “Best Linear Unbiased Estimate” (BLUE)\n\nBut, there is huge variance on green.\nMain idea of regularization:\n\nWhat if we shrink the COEFFICIENT value in the OLS regression\nThis will obviously introduce more bias\n\nBut, as we will see, this almost always reduces out-of-sample variance\n\nThis is also referred to as “shrinkage”\n\n\n\n\n25 \n\n\n\nWhy did we just do this?\nIn general (VERY VERY OFTEN) shrinking our coefficients will improve out-of-sample performance\n\nIt is so important that in big data and ML applications, almost everything is regularized in some way.\n\n\n\n\n26 Shrinkage estimators: general form\n\n\n27 Shrinkage estimators: Lagrangian form\n\n\n28 Ridge regression\n\n\n29 Ridge Regression intuition\n\n\n30 Solution to Ridge\n\n\n31 Note about coefficient scaling\n\n\n32 LASSO: Least Absolute Shrinkage and Selection Operator\n\n\n\n33 LASSO: Not just another penalty.\n\n\n34 LASSO and variable selection\n\nWhen solved, LASSO may result in estimates of exactly zero for some parameters.\n\nIn other words, LASSO shrinks parameter estimates, and shrinks some to zero\nThis results in automated variable selection .\n\nSwitch to VS Code, notebook lectures/03_Regulariation/01_ridge_and_lasso.ipynb\n\n\n\n35 So what did we just do?\n\n\n36 Solution: reduce bias by using OLS after\n\n“post-LASSO regression”\n\nUse LASSO to estimate the main model, letting it do variable selection\nEstimate the main model with selected variables from step 1. This time use OLS.\n\nSwitch back to our notebook where we will implement this.\n\n\n\n37 Appendix\n\n\n38 Elastic Net: LASSO with a ridge\n\n\n39 IV and LASSO\n\n\n40 Motivation\n\n\n41 Many possible instruments in big data contexts\n\n\n42 How many instruments?\n\nAsymptotic theory:\n\nIF all instruments are exogenous and relevant, then they contain useful information.\nFor the most efficient (low variance) estimator, we should use all available information. Question is just how to combine it.\n2SLS can help recover optimal combination of those instruments.\n\nBut, we often have finite samples\nAnd, having too many instruments can introduce bias!\n\nHence, we need some well-thought-out model selection approach.\n\n\n\n\n43 Correct for overfitting-bias with LASSO\n\nIf bias arises from overfitting the first stage, we could try to limit overfitting.\n\nBelloni, Chen, Chernozhukov, Hansen (2012 Econometrica): LASSO!\n\nRough idea:\n\nEstimate first stage via LASSO (with specific penalization scheme)\nUse instruments selected via LASSO in first stage estimated via OLS\nUse fitted values in second stage\n\n\n\n\n44 Many instruments, many controls\n\n\n45 IV and not LASSO\nThe world isn’t always sparse\n\n\n46 Other options for the first stage\n\n\n47 Deep IV: setup\n\n\n48 Deep IV: idea"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#automatic-feature-selection-with-lasso-regression",
    "href": "Machine_Learning/regularization_with_lasso.html#automatic-feature-selection-with-lasso-regression",
    "title": "14  Regularization with Lasso",
    "section": "14.1 Automatic feature selection with LASSO regression",
    "text": "14.1 Automatic feature selection with LASSO regression\nIn this notebook we will learn how LASSO (Least Absolute Shrinkage and Selection Operator) regression works and how it can assist in automatically selecting which variables should be included using a Cross-Validation perspective.\n\n14.1.0.1 Start by importing packages\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LassoCV, Lasso\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport statsmodels\nfrom statsmodels.api import OLS\n\n\n\n14.1.0.2 Load dataset and inspect it\nAgain we’re going to use our diabetes dataset. Inspect it again just to remind yourself what is in it.\n\ndiabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nfeature_names = diabetes.feature_names\n\nprint(diabetes['DESCR'])\nprint(feature_names)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n\n\n\n\n14.1.0.3 Select subset of data\nTo speed up calculation, we’re going to just use the first 150 observations using numpy slice notation to grab them out of the X, y\n\nX = X[:150]\ny = y[:150]\n\nprint(X)\n\n[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n  -0.01764613]\n [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n  -0.09220405]\n [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n  -0.02593034]\n ...\n [-0.05637009 -0.04464164  0.09295276 ...  0.02545259  0.02606052\n   0.04034337]\n [-0.06000263  0.05068012  0.01535029 ... -0.00259226 -0.03074792\n  -0.0010777 ]\n [-0.04910502  0.05068012 -0.00512814 ...  0.07120998  0.06123763\n  -0.03835666]]\n\n\n\n\n14.1.0.4 Run OLS first (for comparison)\nRemember the standard Sklearn model steps:\n\ncreate the model object\ncall the object’s fit method.\nuse the fitted model to predict something.\nassess the predictions.\n\n\n# Create linear regression object\nmodel_ols = linear_model.LinearRegression()\n\n# Train the model using the training sets\nmodel_ols.fit(X, y)\n\n# Make predictions using the testing set\ny_hat = model_ols.predict(X)\n\n# The coefficients\nprint(\"Coefficients: \\n\", model_ols.coef_)\n\n# The mean squared error\nprint(\"Mean squared error:\", mean_squared_error(y, y_hat))\n\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination:\", r2_score(y, y_hat))\n\nCoefficients: \n [ -67.3322587  -369.98803486  445.91969019  324.49756622   89.12828579\n -370.37260059 -263.56792004  123.19006966  579.0388831    89.90418524]\nMean squared error: 2662.075876125911\nCoefficient of determination: 0.5298596601593836\n\n\n\n\n14.1.0.5 Do it again in the econometrics style\nRecall that the package statsmodels is closer to the econometrician’s way of doing things. We’re going to quickly repeat the steps above but with Statsmodels so we can view it in a nice table form.\n\nx_with_constant = statsmodels.api.add_constant(X)\nresult = OLS(y, x_with_constant).fit().summary()\n\nprint(result)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.530\nModel:                            OLS   Adj. R-squared:                  0.496\nMethod:                 Least Squares   F-statistic:                     15.67\nDate:                Thu, 29 Dec 2022   Prob (F-statistic):           1.54e-18\nTime:                        14:05:48   Log-Likelihood:                -804.36\nNo. Observations:                 150   AIC:                             1631.\nDf Residuals:                     139   BIC:                             1664.\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.9381      4.568     33.483      0.000     143.907     161.969\nx1           -67.3323    106.167     -0.634      0.527    -277.243     142.579\nx2          -369.9880    112.515     -3.288      0.001    -592.451    -147.525\nx3           445.9197    119.643      3.727      0.000     209.365     682.475\nx4           324.4976    117.440      2.763      0.007      92.298     556.697\nx5            89.1283    833.489      0.107      0.915   -1558.827    1737.084\nx6          -370.3726    694.067     -0.534      0.594   -1742.667    1001.922\nx7          -263.5679    398.014     -0.662      0.509   -1050.513     523.377\nx8           123.1901    274.716      0.448      0.655    -419.972     666.352\nx9           579.0389    303.265      1.909      0.058     -20.570    1178.648\nx10           89.9042    105.344      0.853      0.395    -118.380     298.189\n==============================================================================\nOmnibus:                        0.798   Durbin-Watson:                   1.871\nProb(Omnibus):                  0.671   Jarque-Bera (JB):                0.431\nSkew:                          -0.042   Prob(JB):                        0.806\nKurtosis:                       3.248   Cond. No.                         266.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n14.1.0.6 Plot y and y_hat\nLet’s also plot y and y_hat compared to one of the most important variables, BMI. We’ll see both y and y_hat resemble each other.\n\n# Plot outputs (comparing 1 variable (BMI in column 3) to y and y_hat\nplt.scatter(X[:, 3], y, color=\"black\")\nplt.scatter(X[:, 3], y_hat, color=\"blue\")\n\n&lt;matplotlib.collections.PathCollection at 0x1a2e0cf3b50&gt;"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#switch-to-lasso",
    "href": "Machine_Learning/regularization_with_lasso.html#switch-to-lasso",
    "title": "14  Regularization with Lasso",
    "section": "14.2 Switch to LASSO",
    "text": "14.2 Switch to LASSO\nNow that we’ve spent all this time setting up our python environment and getting sklearn, it’s almost a trivial step in many cases to try out the latest-and-greatest model.\n\n14.2.0.1 Create a LASSO model object\nToday’s goal, however, is to do Lasso on this same dataset. To start, lets create a Lasso object. Notice that we are not setting the alpha/gamma value when we create it.\n\nmodel_lasso = Lasso(alpha=1.0, random_state=0, max_iter=10000) # Note, alpha is set by default to 1.0 so we could have omitted it here (though I kept it in to make it clear)\nprint(model_lasso)\n\nLasso(max_iter=10000, random_state=0)\n\n\n\n\n14.2.0.2 Fit the LASSO\nCall the lasso.fit() method.\n\nmodel_lasso.fit(X, y)\nprint(model_lasso)\n\nLasso(max_iter=10000, random_state=0)\n\n\n\ny_hat_lasso = model_lasso.predict(X)\nprint('y_hat_lasso', y_hat_lasso)\n\ny_hat_lasso [172.63694312 112.56436625 162.13985803 156.08973773 129.74383298\n 125.28140937 115.61884338 136.59052179 159.6287421  185.05063898\n 106.82661307 118.62966678 132.01651185 164.27673474 132.32978307\n 159.52719536 180.05860468 163.52345725 141.12618691 142.73726374\n 132.32597226 118.12828252 126.6122036  214.79358554 149.32105261\n 154.5286222  115.96885102 158.72385667 145.06984361 171.08808536\n 150.95110473 120.84369573 180.56864605 131.55817157 113.02543458\n 150.9215012  176.31531879 159.41171407 194.18582819 160.99349743\n 153.08379216 115.57213577 144.18353926 128.89860148 178.86126303\n 136.35519126 144.6493225  126.12767637 118.19733477 167.06059041\n 140.67767067 153.45410491 141.38478192 134.18782266 141.05570494\n 113.76852531 172.85209994 114.2633392  134.72604324 158.58764454\n 130.25790072 165.17534412 117.92607201 129.94775178 137.1011572\n 160.31880191 146.17550981 142.41117762 138.4133546  120.17269536\n 110.18834819 168.45389802 180.95757677 143.6382227  152.37093049\n 145.2863145  145.72204735 114.03536748 152.50980767 115.92610137\n 162.78887649 142.77074648 115.74271899 146.82848624 115.40891135\n 152.17529034  96.39049597 164.92236812 127.76402029 129.58254816\n 117.3524975  176.77457893 156.18693227 122.55104724 130.97049958\n 136.93397481 175.47713603 172.74225341 146.90826663 139.35118138\n 168.49953863 128.13758671 155.27499811 156.22880933 143.1622842\n 138.64434869 109.92229447 142.60610808 178.34497712 138.46006222\n  95.66797317 142.68220906 146.5611342  171.24021892 213.87120181\n 181.99208822 172.64335057 174.81696371 155.57294193 149.30073174\n 136.6525253  162.03454774 181.96440315 164.55326954 150.19022137\n 178.46763855 100.6840604  151.64942745 124.00006657 168.88710262\n 181.94923866 116.17042024 140.48211467 110.62902725 150.69250972\n 182.86260255 106.01067503 168.7211344  187.55905836 185.62473912\n 141.42848293 178.66151776 170.32853183 136.40099477 163.90515518\n 175.27393727 170.22208613 182.43176328 142.61890724 172.02344691]\n\n\n\n\n14.2.0.3 Plot it too to compare it with the OLS plot from above\nWhat do you see. Is this expected?\n\n# Plot outputs\nplt.scatter(X[:, 3], y, color=\"black\")\nplt.scatter(X[:, 3], y_hat_lasso, color=\"blue\")\n\nplt.show()\n\n\n\n\n\n\n14.2.0.4 Compare the actual coefficients created\nClass question: How are they different? And how are they similar?\n\nprint(model_lasso.coef_)\nprint(model_ols.coef_)\n\n[  0.          -0.         239.9258791    0.          -0.\n  -0.          -0.           0.         373.07866685   0.        ]\n[ -67.3322587  -369.98803486  445.91969019  324.49756622   89.12828579\n -370.37260059 -263.56792004  123.19006966  579.0388831    89.90418524]"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#exercise-1",
    "href": "Machine_Learning/regularization_with_lasso.html#exercise-1",
    "title": "14  Regularization with Lasso",
    "section": "14.3 Exercise 1",
    "text": "14.3 Exercise 1\nUse a loop to identify the best value of alpha, as measured by r-squared.\nWrite all of the alphas and associated r2 into a dictionary\nDiscussion question for once you’re done: what was the optimal alpha and why does this make sense? How does this compare to OLS? Why is it that way?\n# Starter code: keyt parts omitted.\nscores = {}\nalphas = np.logspace(-5, -0.05, 30)\nfor SOMETHING in SOMETHING_ELSE:\n    model_lasso = Lasso(alpha=alpha, random_state=0, max_iter=10000)\n    # LINE OMIITTED\n    # LINE OMIITTED\n    r2 = r2_score(y, y_hat_lasso)\n    print('R2 for alpha ' + str(alpha) + ': ' + str(r2))\n    scores.append(r2)\n\n# Quick way to get the value from the highest-valued dictionary entry\nbest_alpha = max(scores, key=scores.get)"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#exercise-1-answer",
    "href": "Machine_Learning/regularization_with_lasso.html#exercise-1-answer",
    "title": "14  Regularization with Lasso",
    "section": "14.4 Exercise 1 Answer",
    "text": "14.4 Exercise 1 Answer\n\n# Exercise 1 Answer Code\nscores = {}\nalphas = np.logspace(-5, -0.05, 30)\nfor alpha in alphas:\n    model_lasso = Lasso(alpha=alpha, random_state=0, max_iter=10000)\n    model_lasso.fit(X, y)\n    y_hat_lasso = model_lasso.predict(X)\n    r2 = r2_score(y, y_hat_lasso)\n    scores[alpha] = r2\n\n# Quick way to get the value from the highest-valued dictionary entry\nbest_alpha = max(scores, key=scores.get)\n\nprint('best_alpha', best_alpha)\n\nbest_alpha 1e-05"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#operationalizing-cv-with-gridsearch",
    "href": "Machine_Learning/regularization_with_lasso.html#operationalizing-cv-with-gridsearch",
    "title": "14  Regularization with Lasso",
    "section": "14.5 Operationalizing CV with GridSearch",
    "text": "14.5 Operationalizing CV with GridSearch\nIt seems a little weird to be automatically finding the best model. If we were just applying this to the dataset a single time, this would indeed be p-hacking to the extreme. However, showing its performance on UNSEEN data is quite the opposite of p-hacking.\nHere, we’re going to operationalize our method for finding th ebest model by using GridSearch. We are going to test a variety of different alphas, similar to above. Define them here using numpy logspace:\n\nalphas = np.logspace(-3, -0.5, 30)\nalphas\n\narray([0.001     , 0.00121957, 0.00148735, 0.00181393, 0.00221222,\n       0.00269795, 0.00329034, 0.00401281, 0.0048939 , 0.00596846,\n       0.00727895, 0.0088772 , 0.01082637, 0.01320352, 0.01610262,\n       0.01963828, 0.02395027, 0.02920904, 0.03562248, 0.04344412,\n       0.05298317, 0.06461671, 0.07880463, 0.0961078 , 0.11721023,\n       0.14294613, 0.17433288, 0.21261123, 0.25929438, 0.31622777])\n\n\nWe are going to be passing this range of tuning parameters to a GridSearch function that will test which works best when cross-validation methods are applied. First though, we have to put the alphas into the form the GridSearchCV funciton Expects, which is a list of dictionaries.\n\ntuning_parameters = [{'alpha': alphas}]\n\nRecall that CV works by calculating the fit quality of different folds of the training data. Here we will just use 5 folds. GridSearchCV will automatically implement the folding and testing logic.\n\nn_folds = 5\n\n\n14.5.0.1 Create the lasso_cv object from the lasso object\nFinally, we have all our objects ready to pass to the GridSearchVC function which will Give us back a classifier object. Notice that we’re reusing that model_lasso objectg we created above. The difference is that we will be systematically handing different parameters from the tuning_parameters list into the model_lasso object.\n\nmodel_lasso_cv = GridSearchCV(model_lasso, tuning_parameters, cv=n_folds, refit=False)\n\n\n\n14.5.0.2 Fit the lasso_cv object\nWhen we call the model_lasso_cv.fit() method, we will iteratively be calling the Lasso.fit() with different permutations of tuned parameters and then will return the classifier with the best CV fit.\n\nmodel_lasso_cv.fit(X, y)\n\nGridSearchCV(cv=5,\n             estimator=Lasso(alpha=0.8912509381337456, max_iter=10000,\n                             random_state=0),\n             param_grid=[{'alpha': array([0.001     , 0.00121957, 0.00148735, 0.00181393, 0.00221222,\n       0.00269795, 0.00329034, 0.00401281, 0.0048939 , 0.00596846,\n       0.00727895, 0.0088772 , 0.01082637, 0.01320352, 0.01610262,\n       0.01963828, 0.02395027, 0.02920904, 0.03562248, 0.04344412,\n       0.05298317, 0.06461671, 0.07880463, 0.0961078 , 0.11721023,\n       0.14294613, 0.17433288, 0.21261123, 0.25929438, 0.31622777])}],\n             refit=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Lasso(alpha=0.8912509381337456, max_iter=10000,\n                             random_state=0),\n             param_grid=[{'alpha': array([0.001     , 0.00121957, 0.00148735, 0.00181393, 0.00221222,\n       0.00269795, 0.00329034, 0.00401281, 0.0048939 , 0.00596846,\n       0.00727895, 0.0088772 , 0.01082637, 0.01320352, 0.01610262,\n       0.01963828, 0.02395027, 0.02920904, 0.03562248, 0.04344412,\n       0.05298317, 0.06461671, 0.07880463, 0.0961078 , 0.11721023,\n       0.14294613, 0.17433288, 0.21261123, 0.25929438, 0.31622777])}],\n             refit=False)estimator: LassoLasso(alpha=0.8912509381337456, max_iter=10000, random_state=0)LassoLasso(alpha=0.8912509381337456, max_iter=10000, random_state=0)\n\n\nThe classifier object now has a variety of diagnostic metrics, reporting back on different folds within the Cross Validation. Take a look at them below.\n\nprint('model_lasso_cv keys returned:', model_lasso_cv.cv_results_.keys())\n\nmodel_lasso_cv keys returned: dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_alpha', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])\n\n\nSome relevant results are as below, which we’ll extract and assign to lists.\n\nscores = model_lasso_cv.cv_results_['mean_test_score']\nscores_std = model_lasso_cv.cv_results_['std_test_score']\n\nprint('scores', scores)\nprint('scores_std', scores_std)\n\nscores [0.39984741 0.40067546 0.40167546 0.40287999 0.40432758 0.4056638\n 0.40723916 0.40908292 0.41123606 0.41372907 0.41687352 0.41965693\n 0.42092208 0.42175014 0.42260226 0.42355228 0.42456133 0.42570452\n 0.42696662 0.42907498 0.4315905  0.43258464 0.43298746 0.43209091\n 0.42958333 0.42560295 0.41929547 0.40929219 0.39589098 0.37593936]\nscores_std [0.11754727 0.11729368 0.11699041 0.11662935 0.11620183 0.11559979\n 0.11487707 0.11400269 0.11298083 0.11180212 0.11051429 0.10929125\n 0.10826799 0.10702882 0.10566315 0.10406309 0.10222666 0.10004269\n 0.0974447  0.09440138 0.09082211 0.08770956 0.08378049 0.07835987\n 0.07219072 0.06557553 0.05779547 0.0494723  0.04611854 0.04981942]"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#exercise-2",
    "href": "Machine_Learning/regularization_with_lasso.html#exercise-2",
    "title": "14  Regularization with Lasso",
    "section": "14.6 Exercise 2:",
    "text": "14.6 Exercise 2:\nWith your table, explore the scores and alphas lists we’ve created. Identify which alpha is the best, based on the MSE score returned. A challenge here is that sklearn gave us the scores as a list rather than a dictionary (as we built above), so you will need to use the list to create the dictionary.\nOne way to consider doing this would be to create a for loop to iterate through a range(len(scores)): object, saving the alphas and scores to a new dictionary, as in the starter code below.\nSave the optimal alpha as a new variable called chosen_alpha.\noutput_dict = {}\nfor i in OMITTED_CODE:\n    output_dict[alphas[i]] = scores[i]\n    \nbest_alpha = max(output_dict, key=output_dict.get)\n\nprint('best_alpha', best_alpha)"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#exercise-2-answer",
    "href": "Machine_Learning/regularization_with_lasso.html#exercise-2-answer",
    "title": "14  Regularization with Lasso",
    "section": "14.7 Exercise 2 Answer",
    "text": "14.7 Exercise 2 Answer\n\n# Exercise 2 Code\n\noutput_dict = {}\nfor i in range(len(scores)):\n    output_dict[alphas[i]] = scores[i]\n    \nbest_alpha = max(output_dict, key=output_dict.get)\n\nprint('best_alpha', best_alpha)\n\n\nbest_alpha 0.07880462815669913\n\n\n\n14.7.0.1 Use the built-in attributes to get the best alpha\nFortunately, the authors provide a useful best_params_ attribute.\n\nprint('best_parameters:', model_lasso_cv.best_params_)\n\nbest_parameters: {'alpha': 0.07880462815669913}\n\n\nExtract the best alpha, which we will use later.\n\nchosen_alpha = model_lasso_cv.best_params_['alpha']\nprint('chosen_alpha', chosen_alpha)\n\nchosen_alpha 0.07880462815669913\n\n\n\n\n14.7.0.2 Rerun LASSO with the best alpha\nNow we can rerun a vanilla (no CV) version of Lasso with that specific alpha. This will return, for instance, a .coef_ list.\n\nmodel_lasso_cv_2 = Lasso(alpha=chosen_alpha, random_state=0, max_iter=10000).fit(X, y)\n\nprint(\"coefficients\", model_lasso_cv_2.coef_)\n\ncoefficients [ -26.87410362 -318.07808453  427.47324303  272.29570713   -0.\n -181.31265355 -262.37184106    0.          613.14932629   71.09561387]\n\n\nSimply looking at the coefficients tells us which are to be included. Question: How will we know just by looking?\n\n\n14.7.0.3 Extract the feature names and colum indices of the features that Lasso has selected.\n\nselected_coefficient_labels = []\nselected_coefficient_indices = []\nfor i in range(len(model_lasso_cv_2.coef_)):\n    print('Coefficient', feature_names[i], 'was', model_lasso_cv_2.coef_[i])\n    if abs(model_lasso_cv_2.coef_[i]) &gt; 0:\n        selected_coefficient_labels.append(feature_names[i])\n        selected_coefficient_indices.append(i)\n\nCoefficient age was -26.874103620938673\nCoefficient sex was -318.0780845318727\nCoefficient bmi was 427.4732430327188\nCoefficient bp was 272.2957071277454\nCoefficient s1 was -0.0\nCoefficient s2 was -181.31265355198863\nCoefficient s3 was -262.371841059376\nCoefficient s4 was 0.0\nCoefficient s5 was 613.1493262893765\nCoefficient s6 was 71.09561386767885\n\n\nThis process led us to the following selected_coefficient_labels:\n\nprint('selected_coefficient_labels', selected_coefficient_labels)\n\nselected_coefficient_labels ['age', 'sex', 'bmi', 'bp', 's2', 's3', 's5', 's6']\n\n\n\n\n14.7.0.4 Plot the scores versus the alphas\nFor fun, let’s plot the alphas, scores and a confidence range. What does this show us about the optimal alpha and how it varies with score?\n\nplt.figure().set_size_inches(8, 6)\nplt.semilogx(alphas, scores)\n\n\n\n\nA fun aspect of the k-fold approach is you can get a measure of the std_errors involved. Plot those below.\n\nstd_error = scores_std / np.sqrt(n_folds)\n\nplt.semilogx(alphas, scores + std_error, 'b--')\nplt.semilogx(alphas, scores - std_error, 'b--')\n\n\n\n\n\n\n14.7.0.5 Plot the confidence band and the maximum score\nalpha=0.2 controls the translucency of the fill color\n\nplt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n\nplt.ylabel('CV score +/- std error')\nplt.xlabel('alpha')\nplt.axhline(np.max(scores), linestyle='--', color='.5')\nplt.xlim([alphas[0], alphas[-1]])\n\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#switch-back-to-slides",
    "href": "Machine_Learning/regularization_with_lasso.html#switch-back-to-slides",
    "title": "14  Regularization with Lasso",
    "section": "14.8 Switch back to slides",
    "text": "14.8 Switch back to slides"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#post-lasso",
    "href": "Machine_Learning/regularization_with_lasso.html#post-lasso",
    "title": "14  Regularization with Lasso",
    "section": "14.9 Post-LASSO",
    "text": "14.9 Post-LASSO\nFinally, now that we have our selected labels, we can use them to select the numpy array columns that we want to use for a post-LASSO run.\n\nnew_x = X[:, selected_coefficient_indices]\nnew_x = statsmodels.api.add_constant(new_x)\nprint('new_x', new_x)\n\nnew_x [[ 1.          0.03807591  0.05068012 ... -0.04340085  0.01990749\n  -0.01764613]\n [ 1.         -0.00188202 -0.04464164 ...  0.07441156 -0.06833155\n  -0.09220405]\n [ 1.          0.08529891  0.05068012 ... -0.03235593  0.00286131\n  -0.02593034]\n ...\n [ 1.         -0.05637009 -0.04464164 ... -0.02867429  0.02606052\n   0.04034337]\n [ 1.         -0.06000263  0.05068012 ...  0.019187   -0.03074792\n  -0.0010777 ]\n [ 1.         -0.04910502  0.05068012 ... -0.06917231  0.06123763\n  -0.03835666]]\n\n\nPlug this new x matrix into our statsmodels OLS function and print that out.\n\nresult = OLS(y, new_x).fit().summary()\nprint(result)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.529\nModel:                            OLS   Adj. R-squared:                  0.502\nMethod:                 Least Squares   F-statistic:                     19.79\nDate:                Thu, 29 Dec 2022   Prob (F-statistic):           8.67e-20\nTime:                        14:05:51   Log-Likelihood:                -804.49\nNo. Observations:                 150   AIC:                             1627.\nDf Residuals:                     141   BIC:                             1654.\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.8679      4.503     33.944      0.000     143.965     161.771\nx1           -66.9996    105.470     -0.635      0.526    -275.506     141.507\nx2          -366.0229    111.491     -3.283      0.001    -586.432    -145.614\nx3           444.0490    118.802      3.738      0.000     209.186     678.912\nx4           318.1611    115.915      2.745      0.007      89.005     547.317\nx5          -231.1543    104.191     -2.219      0.028    -437.132     -25.177\nx6          -296.7442    110.058     -2.696      0.008    -514.321     -79.168\nx7           635.5421    123.804      5.133      0.000     390.790     880.294\nx8            98.0388    103.296      0.949      0.344    -106.171     302.248\n==============================================================================\nOmnibus:                        0.803   Durbin-Watson:                   1.880\nProb(Omnibus):                  0.669   Jarque-Bera (JB):                0.437\nSkew:                          -0.051   Prob(JB):                        0.804\nKurtosis:                       3.244   Cond. No.                         35.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#class-discussion",
    "href": "Machine_Learning/regularization_with_lasso.html#class-discussion",
    "title": "14  Regularization with Lasso",
    "section": "14.10 Class discussion",
    "text": "14.10 Class discussion\nHow does the r-squared of this model compare to the one we did at the start of the lecture?\nGiven the above, how is the LASSO approach better than a vanilla OLS?\nLook at the adjusted R-squared. How does that compare across models. In what ways is the adjusted R-squared similar the CV approach?"
  },
  {
    "objectID": "Machine_Learning/neural_networks_intro.html",
    "href": "Machine_Learning/neural_networks_intro.html",
    "title": "15  Neural networks",
    "section": "",
    "text": "Multi-layer perceptrons and convolutional neural nets.\n\n\n16 Logistical comments and agenda\n\nAfter today, Ali will resume lectures for 2 more days\nAgenda:\n\nNeural-nets example\nNN theory\nNN in python\n\n\n\n\n17 \nhttps://www.youtube.com/watch?v=aQwqD5cB2ck\n\n\n18 What is this magic?\n\nBut first, a historical comic.\nThis comic is &gt; 5 years old and the research team was successful.\n\n\n19 Image analysis: from SVMs to Neural Nets to CONVOLUTIONAL Neural Nets\n\n\nRecall that we used Support Vector Machines to categorize these digits.\n\nThese were 8x8 images, but we flattened them into 64 independent features.\nWe’ve also made great progress using Neural Nets to expand beyond just SVM.\n\n\nConvolutional Neural Net, conceptual diagram:\n\n\n\n20 Neurons - Introduction\n\nOur brain makes models of problems through networks of neurons.\n\nNeuroscience in 6 words: “Neurons that fire together wire together.”\n\nEach network takes a set of inputs and fires under certain conditions.\n\nWhat if we mimic that process in ML?\n\n\n\n\n21 Neural networks for regression\nConnect multiple layers of neurons. Often the covariates are considered an input ‘layer.’\nDeterministic function\n\n\n22 Neural networks: learning\n\n\n23 Graphical mapping of the functions\n\nLet’s start with an example of relating dosage to efficacy\nWe define a simple NN with 1 hidden layer and two perceptrons.\n\nEach perceptron is defined as:\n\n\n\n\n\n\n24 How is this fit? Start with some random choices and iterate via CV to find the best coefficients\n\n\n\n\n25 More specifically how is this fit? Back propagation (sec 11.4 in text)\n\n\n26 Intuitively, what is gradient descent?\n\nWe have a multidimensional parameter space. Calculate where the slope downwards is the steepest.\n\nTraverse part of the way down that slope, then recalculate derivatives.\nIteratively do this until you find the bottom.\n\n\n\n\n\n27 Simplified example of a neural net\n\n\n\nOnce trained, the neural net basically “combines squiggles” by adding up the fitted curves\n\n\n28 More complexity can be added to improve predictions\n\n\n\n29 Code Example: Analyzing imagery features\nBased on 500+ images of potentially malignant tumors, extracted variables such as radius, concavity, spacing, etc.\nCompared this to a medical test that identified if the tumor was in fact malignant.\nCODE TIME: Switch to VS Code.\n\n\n\n30 What did we just do?\n\nThe image below is the coefficients of the neurons in our network for different variables (vertical) and network layers (horizontal)\n\n\n\n31 Neural networks: challenges\n\nThrowing a bunch of nodes in layers won’t necessarily work well:\n\nIf you have lots of layers, it will take a long time to train\nSimply adding more nodes doesn’t always increase predictive accuracy\n\nUse problem knowledge to intelligently construct structure of network\n\nWhat about problems that require very large data?\n\nMaybe we want to impose the same weights in different parts e.g. “Convolutional Neural Networks”\n\n\n\n\n\n32 Convolutional Neural Nets\n\n\n33 In our SVM, we were “throwing away” the spatial information by flattening it.\n\nWhat if we try to keep that spatial information? Neural Nets apply can do this!\nHowever, traditional Neural Nets like we’ve just discussed in Code may break down under the huge information of images.\n\n\n34 Image analysis and CNNs.\nImagine an input image, where we’re trying to classify the image (rather than a pixel).\nSay we have 3 bands for that image, and the image is 100x100 pixels. That’s 30,000 inputs.\nSuppose we have one hidden layer with 10 neurons.\nIf we use a fully connected network (like in the code we just made), we would have 300,000 weights and 10 offsets to learn for that hidden layer. Too many.\n\n\n\n35 What are convolutions?\n\n\nhttps://miro.medium.com/max/2400/1*ciDgQEjViWLnCbmX-EeSrA.gif\n\n\n36 Layer types in CNNs\n\nConvolution layer:\n\nConstruct one or more “filters” that get passed over the pixel\nEach filter has a window of say 5x5 pixels. It moves with some “stride” length over the image.\nNeed to chose activation function (sign, linear, logistic)\n\n\n\n\n37 Fitting convolutions into neural nets\nNow perceptrons have convolution kernels as their definition\nSame back-propagation gradient descent method works to solve this\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n38 Layer types in CNNs\n\nThe convolutions can also be added to the rest of a fully-connected Neural Net\n\n\n39 Research advance I’m currently working on: Creating “reduced-form” spatial regression that can work on billion+ observations.\n\nGridded data preserve spatial structure\n\nNearby cells are highly correlated\nThe actual pattern may be a good predictor\n\nCan use 2-dimensional convolutions to express this structure\n\nE.g., identify what is the relationship between two variables as their distance increases via a flexible parametric form\n\n\nB.  Expression in 2- and 3-dimensions\nA.  Example 1-dimensional adjacency relationships\n\nThis is one of the parametric relationships we solve for\n\n\n\n40 Gaussian kernel\nSize = 21, Sigma 4\nApply this definition of spatial effect to each of the different land-use classes.\n\n\n\n41 Preliminary results\n\nGreatly outperforms the look-up table approach (IPCC method)\nEdge effects definitely exist\n\nBut still working to identify exact structure\n\n\n\n\n42 Appendix\nAdditional slides on own research\n\n\n\n43 Spatial results\n\nIncreases precision over linear model\nCan test specific edge-type hypotheses\n\nNote that marginal impact analysis of edge type is tricky\nNote also these use the LassoLarsCV method\n\nPreliminary conclusions:\n\nIntensive crops (class 10) have negative impact on carbon.\nNon-fragmented forests (class 50) has positive impact.\nCities (190) have negative impact.\n\n\n\n\n44 Linear model\n\n\n\n45 Spatial model"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#load-the-data",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#load-the-data",
    "title": "16  Neural Nets Code",
    "section": "16.1 Load the data",
    "text": "16.1 Load the data\nAgain we will use a dataset built-in to Sklearn that includes data related to diagnosing breast cancer.\n\n\ncancer = load_breast_cancer()\n\n# print('Dataset raw object', cancer)\nprint('Dataset description', cancer['DESCR'])\n\nDataset description .. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 569\n\n    :Number of Attributes: 30 numeric, predictive attributes and the class\n\n    :Attribute Information:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry\n        - fractal dimension (\"coastline approximation\" - 1)\n\n        The mean, standard error, and \"worst\" or largest (mean of the three\n        worst/largest values) of these features were computed for each image,\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n        10 is Radius SE, field 20 is Worst Radius.\n\n        - class:\n                - WDBC-Malignant\n                - WDBC-Benign\n\n    :Summary Statistics:\n\n    ===================================== ====== ======\n                                           Min    Max\n    ===================================== ====== ======\n    radius (mean):                        6.981  28.11\n    texture (mean):                       9.71   39.28\n    perimeter (mean):                     43.79  188.5\n    area (mean):                          143.5  2501.0\n    smoothness (mean):                    0.053  0.163\n    compactness (mean):                   0.019  0.345\n    concavity (mean):                     0.0    0.427\n    concave points (mean):                0.0    0.201\n    symmetry (mean):                      0.106  0.304\n    fractal dimension (mean):             0.05   0.097\n    radius (standard error):              0.112  2.873\n    texture (standard error):             0.36   4.885\n    perimeter (standard error):           0.757  21.98\n    area (standard error):                6.802  542.2\n    smoothness (standard error):          0.002  0.031\n    compactness (standard error):         0.002  0.135\n    concavity (standard error):           0.0    0.396\n    concave points (standard error):      0.0    0.053\n    symmetry (standard error):            0.008  0.079\n    fractal dimension (standard error):   0.001  0.03\n    radius (worst):                       7.93   36.04\n    texture (worst):                      12.02  49.54\n    perimeter (worst):                    50.41  251.2\n    area (worst):                         185.2  4254.0\n    smoothness (worst):                   0.071  0.223\n    compactness (worst):                  0.027  1.058\n    concavity (worst):                    0.0    1.252\n    concave points (worst):               0.0    0.291\n    symmetry (worst):                     0.156  0.664\n    fractal dimension (worst):            0.055  0.208\n    ===================================== ====== ======\n\n    :Missing Attribute Values: None\n\n    :Class Distribution: 212 - Malignant, 357 - Benign\n\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n    :Donor: Nick Street\n\n    :Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\nConstruction Via Linear Programming.\" Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets\",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n.. topic:: References\n\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n     San Jose, CA, 1993.\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n     July-August 1995.\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n     163-171."
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#split-into-our-training-and-testing-xy-sets",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#split-into-our-training-and-testing-xy-sets",
    "title": "16  Neural Nets Code",
    "section": "16.2 Split into our training and testing XY sets",
    "text": "16.2 Split into our training and testing XY sets\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#exercise-1-scale-the-data",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#exercise-1-scale-the-data",
    "title": "16  Neural Nets Code",
    "section": "16.3 Exercise 1: Scale the data",
    "text": "16.3 Exercise 1: Scale the data\nThe Multilayer Perceptron (MLP) approach is one of the few that doesn’t automatically scale the data, so let’s do that. Here we will use Numpy to do it manually, though there are alternative built-in methods within scikit-learn.\nIn the code block below, use X_train.mean(axis=0) and similar functions to scale ALL of the X variables so that they have mean 0 and standard deviation 1. HINT: X_train and others are numpy arrays and so you can use fast raster math, e.g., X_train - mean_on_train."
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#exercise-1-answer",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#exercise-1-answer",
    "title": "16  Neural Nets Code",
    "section": "16.4 Exercise 1 Answer",
    "text": "16.4 Exercise 1 Answer\n\n# Exercies 1 Code\n\n# Using numpy functions, compute the mean value per feature on the training set and the STD.\n# May want to remind ourselves what the X_train looks like.\nprint('X_train', X_train)\n\n# Numpy arrays have a .mean() method attached to each array. \n# Below we use that, though note that we have to specify which axis we should calculate the mean on.\n# `axis=0` specifies that we want the mean of each column (which is how the separate variables are stored)\n\nmean_on_train = X_train.mean(axis=0)\n# print('mean_on_train', mean_on_train)\n\n\n# the .std() function is similarily powerful/fast.\nstd_on_train = X_train.std(axis=0)\n# print('std_on_train', std_on_train)\n\n\n# Still using the Numpy awesomeness,\n# subtract the mean, and scale by inverse standard deviation,\n# making it  mean=0 and std=1\nX_train_scaled = (X_train - mean_on_train) / std_on_train\nX_test_scaled = (X_test - mean_on_train) / std_on_train\n\n\nX_train [[1.185e+01 1.746e+01 7.554e+01 ... 9.140e-02 3.101e-01 7.007e-02]\n [1.122e+01 1.986e+01 7.194e+01 ... 2.022e-02 3.292e-01 6.522e-02]\n [2.013e+01 2.825e+01 1.312e+02 ... 1.628e-01 2.572e-01 6.637e-02]\n ...\n [9.436e+00 1.832e+01 5.982e+01 ... 5.052e-02 2.454e-01 8.136e-02]\n [9.720e+00 1.822e+01 6.073e+01 ... 0.000e+00 1.909e-01 6.559e-02]\n [1.151e+01 2.393e+01 7.452e+01 ... 9.653e-02 2.112e-01 8.732e-02]]"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#create-the-mlp-model-object-and-fit-it",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#create-the-mlp-model-object-and-fit-it",
    "title": "16  Neural Nets Code",
    "section": "16.5 Create the MLP model object and fit it",
    "text": "16.5 Create the MLP model object and fit it\nUsing this new scaled training data, we are ready to define a Neural Net, Known here as a Multi-Layer-Perceptron (MLP) classifier. Because this next line hides away millions of other lines of code, you may want to explore it. In VS Code, you can navigate to a function’s definition by placing your cursor in the function and press f-12. Try it in the cell below on the MLPClassifier code! The best documentation is often the code itself.\n\nmlp = MLPClassifier(random_state=0)\n\n# Now fit it with the scaled X and y TRAINING data.\nmlp.fit(X_train_scaled, y_train)\n\nprint(mlp)\n\nMLPClassifier(random_state=0)\n\n\nc:\\Users\\jajohns\\AppData\\Local\\mambaforge\\envs\\8222env1\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn("
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#assess-the-fit",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#assess-the-fit",
    "title": "16  Neural Nets Code",
    "section": "16.6 Assess the fit",
    "text": "16.6 Assess the fit\nNow we assess MLP’s accuracy on the TRAINING and the TESTING data.\nNotice here also I’m introducing another convenient way of combining strings and numbers. The {:.2f} specifies a placeholder for a 2-digit representation of a floating point number. The Format method then places that floating point value into that placeholder.\n\nscore_train = mlp.score(X_train_scaled, y_train)\nscore_test = mlp.score(X_test_scaled, y_test)\n\nprint(\"Accuracy on training set: {:.3f}\".format(score_train))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n\nAccuracy on training set: 0.991\nAccuracy on test set: 0.965"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#plot-the-inputs-and-hidden-layers-of-the-neural-net",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#plot-the-inputs-and-hidden-layers-of-the-neural-net",
    "title": "16  Neural Nets Code",
    "section": "16.7 Plot the inputs and hidden layers of the neural net",
    "text": "16.7 Plot the inputs and hidden layers of the neural net\nIt can be hard perhaps to visualize what exaclty the neural net looks like (there is no coefficients table to simply look at). But here, it is small enough to actually visualize the coefficients within the network.\nBelow, we plot the coeffs_ array to see it.\n\n\nplt.figure(figsize=(20, 5))\nplt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\nplt.yticks(range(30), cancer.feature_names)\nplt.xlabel(\"Columns in weight matrix\")\nplt.ylabel(\"Input feature\")\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#exercies-5.1.2-understanding-which-features-matter-most",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#exercies-5.1.2-understanding-which-features-matter-most",
    "title": "16  Neural Nets Code",
    "section": "16.8 Exercies 5.1.2: Understanding which features matter most",
    "text": "16.8 Exercies 5.1.2: Understanding which features matter most\nOne of the massive challenges in Neural Nets is understanding why exactly it makes the predictions it does. Can you identify which input feature shows the largest positive effect on on cancer diagnosis?\nYou probably can’t make heads or tails of it. Let’s create a greatly simplified version of our neural network to try to see if we can understand it.\nSpecifically create a new MLPClassifier but this time make it have only a single hidden layer. Hint: use f-12 on the MLPClassifier code to see it’s documentation and figure out what new input variable you sohuld specify when calling mlp = MLPClassifier( .......  ). Plot the output coefficients just like above. With only a single layer, the variables become somewhat more interpretable.\nWhich variable now seems to have the largest positive impact?\n\n# Excercise 5.1.2 workspace"
  },
  {
    "objectID": "Spatial_Analysis/spatial_data_intro.html",
    "href": "Spatial_Analysis/spatial_data_intro.html",
    "title": "17  Python for Big, Spatial Data",
    "section": "",
    "text": "Random python-related tweet\n\n\n18 Lecture 3 (of Justin’s lectures)\nWhile we’re waiting, go ahead and get the lecture slides which I’ve just uploaded to canvas. Also pull the latest code from our repository.\n\n\n19 Overview for today\n\nYesterday, we connected to GitHub and gave a brutally quick introduction to Python.\nToday we’re going to:\n\nPickup where we left off\nLearn more about Numpy\nLearn about spatial data in Python\nDo raster-math “at scale”\nIf there’s time, start into our first Machine Learning model\n\n\n\n\n20 Introduction to big and spatial data\n\n\nExample from recent publication in Ecological Economics\n\nCombined both econometrics and “big-data array manipulation”"
  },
  {
    "objectID": "Spatial_Analysis/classification_intro.html",
    "href": "Spatial_Analysis/classification_intro.html",
    "title": "18  Last day of class!",
    "section": "",
    "text": "19 Before the storm\n\n\n20 Agenda\n\nLogistical note: scores for all submitted assignments should be up to date (check Canvas to see if everything looks as expected)\nDiscuss two current events that are shaking the world\nTwo final ML Big-data models\n\nRegression trees\nNa�ve Bayes classification\n\nIntegrating R and Python in one environment\n\n\n\n\n21 AI/ML hits the mainstream\n\n\n22 ChatGPT\n\nThe chatbot is a large language model that can “predict” the best words to respond to any prompt.\n\nBased on OpenAI’s GPT-3.5 model\nThe model includes 175 billion parameters (requiring 800 GB of storage).\n\nUses reinforcement learning on a recurrent neural network\nWhat does this change for us?\n\n\n\n\n\n23 Example using Assignment 1\n\n\n24 What have we done?!\n\nIn the very least, this course has prepped you with the tools to understand and leverage this.\n\nFor example, here’s a tutorial that uses almost exactly our Python setup to create their own Chatbot:\n\nhttps://www.youtube.com/watch?v=C-8sF81k7cY\n\nBased on this tutorial:\n\nhttps://jman4190.medium.com/how-to-build-a-gpt-3-chatbot-with-python-7b83e55805e6\n\n\n\n\n\n25 Classification of land-use, land-cover\n\n\n26 From raw sentinel reflectance data\n\n\n27 Some machine learning approaches retain “understandability”\n\nThroughout this course we’ve seen applications of satellite data\n\nOften, we don’t use the raw reflectance data\nInstead, we use a “remote sensing product” like land-use, land-cover (LULC) maps.\n\nMost LULC maps are created by some form of a “regression tree” based approach\n\nWe’ll talk through those in a moment\nFor now, though, I want to keep it simple with a highly “understandable” model: Na�ve Bayes.\n\n\n\n\n28 Na�ve Bayes\n\n\n\n29 Recommended reading\nJohnson, B. A., & Iizuka, K. (2016). Integrating OpenStreetMap crowdsourced data and Landsat time-series imagery for rapid land use/land cover (LULC) mapping: Case study of the Laguna de Bay area of the Philippines. Applied Geography, 67, 140-149.\n Breiman   , Leo. 2001. Random Forests. Machine Learning 45-1: 5-32. \n\n\n30 LULC classification example\nOpen up 04_Random_Forests/01_lulc_classification.ipynb\n\n\n31 Regression trees and Random forests!\n\n\n32 But first, a quick break for Student Evaluation of Instructor.\n\n\n33 What are these trees?\n\n\n34 Regression trees\nView as decision tree\nView as partition\n\n\n35 Building trees: approach\nIn the most basic implementation, pretty much a brute force approach:\nStart with a tree with one leaf, and compute “loss” (prediction error or impurity)\nFor each variable and each possible split point for that variable, try splitting, compute loss again.\nPick the best possible split from the previous step. Compare to current loss. If the improvement outweighs a complexity penalty, split. Otherwise stop.\nRepeat 1-3 for each leaf in the new tree\nView as decision tree\n\n\n36 Moving beyond a tree\n\nTrees are very flexible and intuitive, but not without their problems:\n\nCan be sensitive to input data � changing a point can give different splits, etc\nMost functions we’re trying to learn aren’t really step functions, are they?\nHow do we quantify uncertainty?\n\nMuch of this can be addressed with bootstrapping.\n\n\n\n37 Bootstrap reminder\n\nFor a sample with n observations:\n\nCreate a bootstrap sample b by sampling n data points with replacement _ _ from our data\nEstimate the model on b\nAverage the estimates over B bootstrap samples, and use that for SE estimation too\n\n\n\n\n38 Bootstrap aggregating (Bagging)\n\nApply the bootstrap procedure to tree-building:\n\nCreate a bootstrap training set b from the original training set\nGrow a tree from b\nRepeat for all b in the set B of bootstrap samples\nTo predict an outcome for a new data point x, predict for each tree, then average (For classification trees, each of the trees vote)\n\n\n\n\n39 From bagging to random forests\n\nWe can go further with the randomization idea in bagging.\nIdea behind both: lower variance in predictions by averaging over different trees.\n\nBagging: Get different trees by randomizing sample\nRandom forests: Bagging plus randomizing which variables you can split on.\n\nEach time you go to split, only consider a randomly selected subset of variables, and choose the best split among those variables.\n\n\n\n40 R and Python together\n\n\n41 Related to a topic very close to me: How do we glue multiple models together?\n\n\n\n42 Let’s see!\nOpen up 06_R_and_Python_together/01_combining_languages.ipynb\n\n\n43 \n\n\n\n44 From python for economists book\nIncorporate  https://aeturrell.github.io/coding-for-economists/coming-from-r.html\nFor those coming from the ’ tidyverse ’ set of packages produced by RStudio, there are very direct Python equivalents. For example, Python has   plotnine   which has the same syntax to R’s   ggplot2  . There’s also   plydata  , which has the same syntax as R’s   dplyr   package. In Python,   matplotlib   and   pandas   are more popular packages for plotting and data analysis, respectively, but those R-style packages are absolutely there if you prefer them. More on other similar packages below."
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#introduction",
    "href": "Spatial_Analysis/lulc_classification.html#introduction",
    "title": "19  Classification of Land Cover",
    "section": "19.1 Introduction",
    "text": "19.1 Introduction\nIn this chapter we will classify data from the Sentinel-2 satellite using a supervised classification approach which incorporates the training data represented as a vector (shapefile). Specifically, we will be using Naive Bayes. Naive Bayes predicts the probabilities that a data point belongs to a particular class and the class with the highest probability is considered as the most likely class. The way they get these probabilities is by using Bayes’ Theorem, which describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature. Naive Bayes is quite fast when compared to some other machine learning approaches (e.g., SVM can be quite computationally intensive). This isn’t to say that it is the best per se; rather it is a great first step into the world of machine learning for classification and regression."
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#preparing-the-dataset",
    "href": "Spatial_Analysis/lulc_classification.html#preparing-the-dataset",
    "title": "19  Classification of Land Cover",
    "section": "19.2 Preparing the dataset",
    "text": "19.2 Preparing the dataset\n\n19.2.0.1 Opening the images\nOur first step is to import the relevant packages. Of note are a couple new ones, namely rasterio and shapely. These are excellent libraries that simplify working with rasters and vectors (respectively). They wrap around GDAL so you don’t have to do it the hard way. They also provide some very useful helper functions, like mask and mapping, which we explicitly import below.\n\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport numpy as np\nfrom shapely.geometry import mapping\n\nNow we need to collect all the Sentinal-2 bands because they come as individual images one per band. Ultimately, we’re going to rewrite them into a multi-band (8 bands) geotiff for later use in regression.\n\nimport os # we need os to do some basic file operations\n\ndata_dir = \"../data/lulc_classification_example/\"\n\nsentinal_fp = os.path.join(data_dir, \"sentinel-2/\")\n\n# If this isn't working it's probably because you have the file saved somewhere else than where sentinal_fp points. \n# Examine the absolute path to investigate.\n\n\na = os.path.abspath(sentinal_fp)\n# find every file in the sentinal_fp directory\nsentinal_band_paths = [os.path.join(sentinal_fp, f) for f in os.listdir(sentinal_fp) if os.path.isfile(os.path.join(sentinal_fp, f))]\nsentinal_band_paths.sort()\nsentinal_band_paths\n\n['../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B01.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B02.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B03.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B04.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B05.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B06.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B07.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B08.tiff']\n\n\nBelow we will create a rasterio dataset object containing all bands in order to use the mask() function and extract pixel values using geospatial polygons.\nWe’ll do this by creating a new raster dataset and saving it for future uses.\n\n# create a products directory within the data dir which won't be uploaded to Github\nworkspace_dir = '../../textbook_workspace'\nimg_dir = os.path.join(workspace_dir, 'generated_lulc')\n\n# check to see if the dir it exists, if not, create it\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n\n# filepath for image we're writing out\nimg_fp = os.path.join(img_dir, 'sentinel_bands.tif')\n\n# Read metadata of first file and assume all other bands are the same\nwith rasterio.open(sentinal_band_paths[0]) as src0:\n    meta = src0.meta\n\n# Update metadata to reflect the number of layers\nmeta.update(count = len(sentinal_band_paths))\n\n# Read each layer and write it to stack\nwith rasterio.open(img_fp, 'w', **meta) as dst:\n    for id, layer in enumerate(sentinal_band_paths, start=1):\n        with rasterio.open(layer) as src1:\n            dst.write_band(id, src1.read(1))\n\nOkay we’ve successfully written it out now let’s open it back up and make sure it meets our expectations:\n\nfull_dataset = rasterio.open(img_fp)\nimg_rows, img_cols = full_dataset.shape\nimg_bands = full_dataset.count\nprint(full_dataset.shape) # dimensions\nprint(full_dataset.count) # bands\n\n(2201, 2629)\n8"
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#exercise",
    "href": "Spatial_Analysis/lulc_classification.html#exercise",
    "title": "19  Classification of Land Cover",
    "section": "19.3 Exercise:",
    "text": "19.3 Exercise:\nLet’s clip the image and take a look at it. In the starter code below, we use the .read() method to read three different bands into a single image that we will plot. We also use numpy slice notation to clip out a smaller part of the array.\nHOWEVER, the image won’t look right on its own. You can kind of tell there is color, but it doesn’t look like it should. That’s because we’re reading in the wrong bands (our eyes are expecting Red, Green and Blue).\nAs a class race, try out different combinations of bands. When you think you’ve got it, raise your hand. When someone gets it, or after 2 minutes, whoever has the best image wins! Hint: if you want to nail this, just look at the sentinel documentation https://gprivate.com/62co2.\n\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n\nband_1 = 1\nband_2 = 2\nband_3 = 3\n\nclipped_img = full_dataset.read([band_1, band_2, band_3])[:, 150:600, 250:1400]\nprint(clipped_img.shape)\nfig, ax = plt.subplots(figsize=(10,7))\nshow(clipped_img[:, :, :], ax=ax, transform=full_dataset.transform) # add the transform arg to get it in lat long coords\n\n(3, 450, 1150)\n\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nOkay looks good! Our raster dataset is ready!\n\n19.3.1 Now our goal is to get the pixels from the raster as outlined in each shapefile.\nOur training data, the shapefile we’ve worked with, contains one main field we care about: + a Classname field (String datatype)\nCombined with the innate location information of polygons in a Shapefile, we have all that we need to use for pairing labels with the information in our raster.\nHowever, in order to pair up our vector data with our raster pixels, we will need a way of co-aligning the datasets in space.\nWe’ll do this using the rasterio mask function which takes in a dataset and a polygon and then outputs a numpy array with the pixels in the polygon.\nLet’s run through an example:\n\nfull_dataset.crs\n\nCRS.from_epsg(4326)\n\n\nOpen up our shapefile and check its crs\n\nshapefile = gpd.read_file(os.path.join(data_dir, 'rcr', 'rcr_landcover.shp'))\nshapefile.crs\n\n&lt;Derived Projected CRS: EPSG:32618&gt;\nName: WGS 84 / UTM zone 18N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 78°W and 72°W, northern hemisphere between equator and 84°N, onshore and offshore. Bahamas. Canada - Nunavut; Ontario; Quebec. Colombia. Cuba. Ecuador. Greenland. Haiti. Jamaica. Panama. Turks and Caicos Islands. United States (USA). Venezuela.\n- bounds: (-78.0, 0.0, -72.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 18N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nRemember the projections don’t match! Let’s use some geopandas magic to reproject all our shapefiles to lat, long.\n\nshapefile = shapefile.to_crs({'init': 'epsg:4326'})\n\nc:\\Users\\jajohns\\AppData\\Local\\mambaforge\\envs\\8222env1\\lib\\site-packages\\pyproj\\crs\\crs.py:130: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\nshapefile.crs\n\n&lt;Geographic 2D CRS: +init=epsg:4326 +type=crs&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- lon[east]: Longitude (degree)\n- lat[north]: Latitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nlen(shapefile)\n\n23\n\n\nNow we want to extract the geometry of each feature in the shapefile in GeoJSON format:\n\n# this generates a list of shapely geometries\ngeoms = shapefile.geometry.values \n\n# let's grab a single shapely geometry to check\ngeometry = geoms[0] \nprint(type(geometry))\nprint(geometry)\n\n# transform to GeoJSON format\nfrom shapely.geometry import mapping\nfeature = [mapping(geometry)] # can also do this using polygon.__geo_interface__\nprint(type(feature))\nprint(feature)\n\n&lt;class 'shapely.geometry.polygon.Polygon'&gt;\nPOLYGON ((-76.67593927883173 34.69487548849214, -76.67573882771855 34.694513199139024, -76.6766693455509 34.69360077384821, -76.67676946161477 34.69421769352402, -76.67593927883173 34.69487548849214))\n&lt;class 'list'&gt;\n[{'type': 'Polygon', 'coordinates': (((-76.67593927883173, 34.69487548849214), (-76.67573882771855, 34.694513199139024), (-76.6766693455509, 34.69360077384821), (-76.67676946161477, 34.69421769352402), (-76.67593927883173, 34.69487548849214)),)}]\n\n\nNow let’s extract the raster values values within the polygon using the rasterio mask() function\n\nout_image, out_transform = mask(full_dataset, feature, crop=True)\nout_image.shape\n\n(8, 18, 13)\n\n\nOkay those looks like the right dimensions for our training data. 8 bands and 6x8 pixels seems reasonable given our earlier explorations.\nWe’ll be doing a lot of memory intensive work so let’s clean up and close this dataset.\n\nfull_dataset.close()\n\n\n\n19.3.2 Building the Training Data for scikit-learn\nNow let’s do it for all features in the shapefile and create an array X that has all the pixels and an array y that has all the training labels.\n\nX = np.array([], dtype=np.int8).reshape(0,8) # pixels for training\ny = np.array([], dtype=np.string_) # labels for training\n\n# extract the raster values within the polygon \nwith rasterio.open(img_fp) as src:\n    band_count = src.count\n    for index, geom in enumerate(geoms):\n        feature = [mapping(geom)]\n\n        # the mask function returns an array of the raster pixels within this feature\n        out_image, out_transform = mask(src, feature, crop=True) \n        \n        # eliminate all the pixels with 0 values for all 8 bands - AKA not actually part of the shapefile\n        out_image_trimmed = out_image[:,~np.all(out_image == 0, axis=0)]\n        \n        # eliminate all the pixels with 255 values for all 8 bands - AKA not actually part of the shapefile\n        out_image_trimmed = out_image_trimmed[:,~np.all(out_image_trimmed == 255, axis=0)]\n        \n        # reshape the array to [pixel count, bands]\n        out_image_reshaped = out_image_trimmed.reshape(-1, band_count)\n        \n        # append the labels to the y array\n        y = np.append(y,[shapefile[\"Classname\"][index]] * out_image_reshaped.shape[0]) \n        \n        # stack the pizels onto the pixel array\n        X = np.vstack((X, out_image_reshaped))        \n\n\n19.3.2.1 Pairing Y with X\nNow that we have the image we want to classify (our X feature inputs), and the land cover labels (our y labeled data), let’s check to make sure they match in size so we can feed them to Naive Bayes:\n\n# What are our classification labels?\nlabels = np.unique(shapefile[\"Classname\"])\nprint('The training data include {n} classes: {classes}\\n'.format(n=labels.size, \n                                                                classes=labels))\n\n# We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels\nprint('Our X matrix is sized: {sz}'.format(sz=X.shape))\nprint('Our y array is sized: {sz}'.format(sz=y.shape))\n\nThe training data include 6 classes: ['Emergent Wetland' 'Forested Wetland' 'Herbaceous' 'Sand'\n 'Subtidal Haline' 'WetSand']\n\nOur X matrix is sized: (598, 8)\nOur y array is sized: (598,)\n\n\nIt all looks good! Let’s explore the spectral signatures of each class now to make sure they’re actually separable since all we’re going by in this classification is pixel values.\n\nfig, ax = plt.subplots(1,3, figsize=[20,8])\n\n# numbers 1-8\nband_count = np.arange(1,9)\n\nclasses = np.unique(y)\nfor class_type in classes:\n    band_intensity = np.mean(X[y==class_type, :], axis=0)\n    ax[0].plot(band_count, band_intensity, label=class_type)\n    ax[1].plot(band_count, band_intensity, label=class_type)\n    ax[2].plot(band_count, band_intensity, label=class_type)\n# plot them as lines\n\n# Add some axis labels\nax[0].set_xlabel('Band #')\nax[0].set_ylabel('Reflectance Value')\nax[1].set_ylabel('Reflectance Value')\nax[1].set_xlabel('Band #')\nax[2].set_ylabel('Reflectance Value')\nax[2].set_xlabel('Band #')\n#ax[0].set_ylim(32,38)\nax[1].set_ylim(32,38)\nax[2].set_ylim(70,140)\n#ax.set\nax[1].legend(loc=\"upper right\")\n# Add a title\nax[0].set_title('Band Intensities Full Overview')\nax[1].set_title('Band Intensities Lower Ref Subset')\nax[2].set_title('Band Intensities Higher Ref Subset')\n\nText(0.5, 1.0, 'Band Intensities Higher Ref Subset')\n\n\n\n\n\nThey look okay but emergent wetland and subtital haline look quite similar! They’re going to be difficult to differentiate.\nLet’s make a quick helper function, this one will convert the class labels into indicies and then assign a dictionary relating the class indices and their names.\n\ndef str_class_to_int(class_array):\n    class_array[class_array == 'Subtidal Haline'] = 0\n    class_array[class_array == 'WetSand'] = 1\n    class_array[class_array == 'Emergent Wetland'] = 2\n    class_array[class_array == 'Sand'] = 3\n    class_array[class_array == 'Herbaceous'] = 4\n    class_array[class_array == 'Forested Wetland'] = 5\n    return(class_array.astype(int))"
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#training-the-classifier",
    "href": "Spatial_Analysis/lulc_classification.html#training-the-classifier",
    "title": "19  Classification of Land Cover",
    "section": "19.4 Training the Classifier",
    "text": "19.4 Training the Classifier\nNow that we have our X matrix of feature inputs (the spectral bands) and our y array (the labels), we can train our model.\nVisit this web page to find the usage of GaussianNaiveBayes Classifier from scikit-learn.\n\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X, y)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\nIt is that simple to train a classifier in scikit-learn! The hard part is often validation and interpretation."
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#predicting-on-the-image",
    "href": "Spatial_Analysis/lulc_classification.html#predicting-on-the-image",
    "title": "19  Classification of Land Cover",
    "section": "19.5 Predicting on the image",
    "text": "19.5 Predicting on the image\nWith our Naive Bayes classifier fit, we can now proceed by trying to classify the entire image:\nWe’re only going to open the subset of the image we viewed above because otherwise it is computationally too intensive for most users.\n\nfrom rasterio.plot import show\nfrom rasterio.plot import show_hist\nfrom rasterio.windows import Window\nfrom rasterio.plot import reshape_as_raster, reshape_as_image\n\n\nwith rasterio.open(img_fp) as src:\n    # may need to reduce this image size if your kernel crashes, takes a lot of memory\n    img = src.read()[:, 150:600, 250:1400]\n\n# Take our full image and reshape into long 2d array (nrow * ncol, nband) for classification\nprint(img.shape)\nreshaped_img = reshape_as_image(img)\nprint(reshaped_img.shape)\n\n(8, 450, 1150)\n(450, 1150, 8)\n\n\nNow we can predict for each pixel in our image:\n\nclass_prediction = gnb.predict(reshaped_img.reshape(-1, 8))\n\n# Reshape our classification map back into a 2D matrix so we can visualize it\nclass_prediction = class_prediction.reshape(reshaped_img[:, :, 0].shape)\n\nBecause our shapefile came with the labels as strings we want to convert them to a numpy array with ints using the helper function we made earlier.\n\nclass_prediction = str_class_to_int(class_prediction)\n\n\n19.5.1 Let’s visualize it!\nFirst we’ll make a colormap so we can visualize the classes, which are just encoded as integers, in more logical colors. Don’t worry too much if this code is confusing! It can be a little clunky to specify colormaps for matplotlib.\n\ndef color_stretch(image, index):\n    colors = image[:, :, index].astype(np.float64)\n    for b in range(colors.shape[2]):\n        colors[:, :, b] = rasterio.plot.adjust_band(colors[:, :, b])\n    return colors\n    \n# find the highest pixel value in the prediction image\nn = int(np.max(class_prediction))\n\n# next setup a colormap for our map\ncolors = dict((\n    (0, (48, 156, 214, 255)),   # Blue - Water\n    (1, (139,69,19, 255)),      # Brown - WetSand\n    (2, (96, 19, 134, 255)),    # Purple - Emergent Wetland\n    (3, (244, 164, 96, 255)),   # Tan - Sand\n    (4, (206, 224, 196, 255)),  # Lime - Herbaceous\n    (5, (34, 139, 34, 255)),    # Forest Green - Forest \n))\n\n# Put 0 - 255 as float 0 - 1\nfor k in colors:\n    v = colors[k]\n    _v = [_v / 255.0 for _v in v]\n    colors[k] = _v\n    \nindex_colors = [colors[key] if key in colors else \n                (255, 255, 255, 0) for key in range(0, n+1)]\n\ncmap = plt.matplotlib.colors.ListedColormap(index_colors, 'Classification', n+1)\n\nNow show the classified map next to the RGB image!\n\nfig, axs = plt.subplots(2,1,figsize=(10,7))\n\nimg_stretched = color_stretch(reshaped_img, [4, 3, 2])\naxs[0].imshow(img_stretched)\n\naxs[1].imshow(class_prediction, cmap=cmap, interpolation='none')\n\nfig.show()\n\nC:\\Users\\jajohns\\AppData\\Local\\Temp\\ipykernel_15156\\1384158104.py:8: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n\n19.5.2 This looks pretty good!\nLet’s generate a map of Normalized Difference Water Index (NDWI) and NDVI just to compare with out output map.\nNDWI is similar to NDVI but for identifying water.\n\nwith rasterio.open(img_fp) as src:\n    green_band = src.read(3)\n    red_band = src.read(4)\n    nir_band = src.read(8)\n    \nndwi = (green_band.astype(float) - nir_band.astype(float)) / (green_band.astype(float) + nir_band.astype(float))\nndvi = (nir_band.astype(float) - red_band.astype(float)) / (red_band.astype(float) + nir_band.astype(float))\n\nSubset them to our area of interest:\n\nndwi = ndwi[150:600, 250:1400]\nndvi = ndvi[150:600, 250:1400]\n\nDisplay all four maps:\n\nfig, axs = plt.subplots(2,2,figsize=(15,7))\n\nimg_stretched = color_stretch(reshaped_img, [3, 2, 1])\naxs[0,0].imshow(img_stretched)\n\naxs[0,1].imshow(class_prediction, cmap=cmap, interpolation='none')\n\nnwdi_plot = axs[1,0].imshow(ndwi, cmap=\"RdYlGn\")\naxs[1,0].set_title(\"NDWI\")\nfig.colorbar(nwdi_plot, ax=axs[1,0])\n\nndvi_plot = axs[1,1].imshow(ndvi, cmap=\"RdYlGn\")\naxs[1,1].set_title(\"NDVI\")\nfig.colorbar(ndvi_plot, ax=axs[1,1])\n\nplt.show()\n\n\n\n\nLooks pretty good! Areas that are high on the NDWI ratio are generally classified as water and areas high on NDVI are forest and herbaceous. It does seem like the wetland areas (e.g. the bottom right island complex) aren’t being picked up so it might be worth experimenting with other algorithms!\nLet’s take a closer look at the Duke Marine Lab and the tip of the Rachel Carson Reserve.\n\nfig, axs = plt.subplots(1,2,figsize=(15,15))\n\nimg_stretched = color_stretch(reshaped_img, [3, 2, 1])\naxs[0].imshow(img_stretched[0:180, 160:350])\n\naxs[1].imshow(class_prediction[0:180, 160:350], cmap=cmap, interpolation='none')\n\nfig.show()\n\nC:\\Users\\jajohns\\AppData\\Local\\Temp\\ipykernel_15156\\1670099218.py:8: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\nThis actually doesn’t look half bad! Land cover mapping is a complex problem and one where there are many approaches and tools for improving a map."
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#testing-an-unsupervised-classification-algorithm",
    "href": "Spatial_Analysis/lulc_classification.html#testing-an-unsupervised-classification-algorithm",
    "title": "19  Classification of Land Cover",
    "section": "19.6 Testing an Unsupervised Classification Algorithm",
    "text": "19.6 Testing an Unsupervised Classification Algorithm\nLet’s also try a unsupervised classification algorithm, k-means clustering, in the scikit-learn library (documentation)\nK-means (wikipedia page) aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\n\nfrom sklearn.cluster import KMeans\n\nbands, rows, cols = img.shape\n\nk = 10 # num of clusters\n\nkmeans_predictions = KMeans(n_clusters=k, random_state=0).fit(reshaped_img.reshape(-1, 8))\n\nkmeans_predictions_2d = kmeans_predictions.labels_.reshape(rows, cols)\n\n# Now show the classmap next to the image\nfig, axs = plt.subplots(1,2,figsize=(15,8))\n\nimg_stretched = color_stretch(reshaped_img, [3, 2, 1])\naxs[0].imshow(img_stretched)\n\naxs[1].imshow(kmeans_predictions_2d)\n\n&lt;matplotlib.image.AxesImage at 0x19a12736320&gt;\n\n\n\n\n\nWow this looks like it was better able to distinguish some areas like the wetland and submerged sand than our supervised classification approach! But supervised usually does better with some tuning, luckily there are lots of ways to think about improving our supervised method.\nAdapted from the wonderful tutorial series by Patrick Gray: https://github.com/patrickcgray"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html",
    "href": "Spatial_Analysis/python_assignment_2.html",
    "title": "20  Exercise 2",
    "section": "",
    "text": "21 Step 6:\nExtract the first array row of the data_array and assign it to y. Assign the rest to X.\n# Step 6 code\nSplit the X and y into testing and training data such that the training data is the first million pixels and the testing data is the next 200,000. Do this using numpy slice notation on the X and y variables you created.\n# Step 7 code\nTo make the code run faster, we are going to use every 10th pixel. We can easily get this via numpy slicing again, using x_train[::10] to get every 10th pixel.\n# Step 8 code\nCreate a Lasso object (using the default penalty term alpha) and fit it to the training data. Create and print out a vector of predicted carbon values. Also print out the score using the lasso object’s .score() method on the TESTING data. Print out the fitted lasso score.\n# Step 9 code\nTo view how our projections LOOK, we can create a predicted matrix on the whole X, reshape it back into the original 2d shape and look at it. You can compare this to the input array to visualize how it looks. Note that this will only work if you name your objects like mine.\n# Step 10 code\nCreate a list of 30 alphas using np.logspace(-1, 3, 30).\nUsing a for loop iterate over those alphas and run the Lasso model like above, but using the alpha values in the loop. Print out the fit score at each step. Using matplotlib, plot how this value changes as alpha changes. Finally, extract the best alpha of the bunch.\n# Step 11 code\nRerun the lasso with that best value and identify all of the coefficiencts that were “selected” ie had non-zero values. Save these coefficient indices and labels to a list.\n# Step 12 code\nUsing Statsmodels, run an OLS version on the selected variables.\nUse print to show the results table.\nWrite a description of any advantages this approach has over vanilla OLS.\n# Step 13 code"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#using-post-lasso-on-large-spatial-data",
    "href": "Spatial_Analysis/python_assignment_2.html#using-post-lasso-on-large-spatial-data",
    "title": "20  Exercise 2",
    "section": "20.1 Using post-LASSO on large spatial data",
    "text": "20.1 Using post-LASSO on large spatial data\nThis assignment will give you a real (active) research topic that I’ve discussed a little bit in class: predicting carbon storage as a function of high-resolution gridded data. In the class google drive you will find all the data you need. I added it just recently so if you don’t have it, be sure to go get it first.\nThis assignment will have you use the automated variable selection approach within LASSO to deal with a common situation in regressions on raster-stacks: we have so much data everything is significant but will lead to massive overfitting. The basic approach used here will involve reading in 2d rasters, flattening them into a 1d column ready to add to a dataframe shaped object, which we will use as our X matrix.\nPlease turn in the completed Notebook (.ipynb) file that includes the results you generate.\nBelow is some starter code along with specific assignment questions.\n\n# Load libraries\nimport numpy as np\nimport os\nfrom osgeo import gdal\nfrom sklearn.linear_model import Lasso\nfrom matplotlib import pyplot as plt\nfrom statsmodels.api import OLS"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-1-download-the-data-and-set-paths",
    "href": "Spatial_Analysis/python_assignment_2.html#step-1-download-the-data-and-set-paths",
    "title": "20  Exercise 2",
    "section": "20.2 Step 1: Download the data and set paths",
    "text": "20.2 Step 1: Download the data and set paths\nDownload the latest data from the class’s google drive. In there, you will need the the files in Data/python_assignment_2 data and assign a relative path to the soyo_tile directory in that assignment directory. It is your task to ensure your script runs in the right location and the data is stored in the right location that this relative path works.\n\n# Step 1 code"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-2-set-raster-paths",
    "href": "Spatial_Analysis/python_assignment_2.html#step-2-set-raster-paths",
    "title": "20  Exercise 2",
    "section": "20.3 Step 2: Set raster paths",
    "text": "20.3 Step 2: Set raster paths\nAssign each of the raster paths in the directory to a dictionary for later use. I’ve included most of the code (so you don’t have to waste your time typing), but add in the missing paths.\n\n# Step 2 code\n\nraster_paths = {}\n\n# First is the dependent varable, Above Ground Carbon (AGB) in tons, measured at 30 meters globally (here it is clipped to a smaller area)\nraster_paths['agb_observed_baccini_2000_30m'] = os.path.join(data_dir, \"agb_observed_baccini_2000_30m.tif\")\n\n# Here are some of the independent variables\nraster_paths['CRFVOL_M_sl1_250m'] = os.path.join(data_dir, \"CRFVOL_M_sl1_250m.tif\")\nraster_paths['HISTPR_250m'] = os.path.join(data_dir, \"HISTPR_250m.tif\")\nraster_paths['OCDENS_M_sl1_250m'] = os.path.join(data_dir, \"OCDENS_M_sl1_250m.tif\")\nraster_paths['PHIHOX_M_sl1_250m'] = os.path.join(data_dir, \"PHIHOX_M_sl1_250m.tif\")\nraster_paths['roughness_30s'] = os.path.join(data_dir, \"roughness_30s.tif\")\nraster_paths['SLGWRB_250m'] = os.path.join(data_dir, \"SLGWRB_250m.tif\")\nraster_paths['SLTPPT_M_sl1_250m'] = os.path.join(data_dir, \"SLTPPT_M_sl1_250m.tif\")\nraster_paths['terrain_ruggedness_index_30s'] = os.path.join(data_dir, \"terrain_ruggedness_index_30s.tif\")\nraster_paths['TEXMHT_M_sl1_250m'] = os.path.join(data_dir, \"TEXMHT_M_sl1_250m.tif\")\nraster_paths['wc2.0_bio_30s_01'] = os.path.join(data_dir, \"wc2.0_bio_30s_01.tif\")\nraster_paths['alt_30s'] = os.path.join(data_dir, \"alt_30s.tif\")\nraster_paths['AWCh1_M_sl1_250m'] = os.path.join(data_dir, \"AWCh1_M_sl1_250m.tif\")\nraster_paths['BDRICM_M_250m'] = os.path.join(data_dir, \"BDRICM_M_250m.tif\")\nraster_paths['BDRLOG_M_250m'] = os.path.join(data_dir, \"BDRLOG_M_250m.tif\")\nraster_paths['BLDFIE_M_sl1_250m'] = os.path.join(data_dir, \"BLDFIE_M_sl1_250m.tif\")"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-3-open-the-rasters",
    "href": "Spatial_Analysis/python_assignment_2.html#step-3-open-the-rasters",
    "title": "20  Exercise 2",
    "section": "20.4 Step 3: Open the rasters",
    "text": "20.4 Step 3: Open the rasters\nOur dependent variable will be 30 meter observations of carbon storage from Baccini et al. (unpublished, but soon to be published) data. The label I assigned in the dictionary above was agb_observed_baccini_2000_30m for this variable. Use gdal.Open, GetRasterBand(1) and ReadAsArray() to read this geotiff as a numpy file\nSide note: If you get an error like: “ERROR 4: This is a BigTIFF file. BigTIFF is not supported by this version of GDAL and libtiff.” make sure you have installed gdal with the mamba method from lecture 1.\n\n# Step 3 code"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-4-define-some-arrays",
    "href": "Spatial_Analysis/python_assignment_2.html#step-4-define-some-arrays",
    "title": "20  Exercise 2",
    "section": "20.5 Step 4: Define some arrays",
    "text": "20.5 Step 4: Define some arrays\nCreate an empty numpy array (or full of zeros) of the right shape to house all our raster data. A very CPU-efficient way of arranging a stack of 2d rasters (which would be 3d once stacked up), is to flatten each 2d raster into a longer 1d array. This will go into our X matrix. In order to create the right sized X matrix, first get the n_obs and n_vars by inspecting the dependent variable raster and the dictionary of inputs above. Note that the n_vars should be the number of independent AND dependent variables.\n\n# Step 4 code"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-5-load-all-the-independent-variables",
    "href": "Spatial_Analysis/python_assignment_2.html#step-5-load-all-the-independent-variables",
    "title": "20  Exercise 2",
    "section": "20.6 Step 5: Load all the independent variables",
    "text": "20.6 Step 5: Load all the independent variables\n\nIterate through the dictionary and load each raster as a 2d array\nflatten it to 1d using the .flatten() method in numpy\nAssign this 1d array to the correct column of the data array. By convention, the depvar will be the first column.\n\nHint, assuming you have arranged your X array in the correct way, it should have observations (pixels) as rows and variables as cols. Given that each flattened array is for one variable and is as long as there are rows, a convenient way of assigning it would be to use numpy slice notation, potentially similar to:\ndata_array[:, column_index_integer]\nThe first colon just denotes the whole row and the column index is an integer you could create pointing to the right row.\nSome incomplete code to get you started is below.\n\n# Step 5 code\n\nfor name, path in raster_paths.items():\n    print('Loading', path)\n    'MISSING STUFF'\n    flattened_raster_array = band.ReadAsArray().flatten()\n    data_array[:, col_index] = flattened_raster_array\n    feature_names.append(name)"
  },
  {
    "objectID": "Appendices/hosting_notes.html#create-a-github.io-page",
    "href": "Appendices/hosting_notes.html#create-a-github.io-page",
    "title": "21  Hosting notes",
    "section": "21.1 Create a github.io page",
    "text": "21.1 Create a github.io page\nCreate a new repository using your username:\n\n\n\nimage.png\n\n\nClone the newly-created repo\n\n\n\nimage.png\n\n\nThen just put the index.html file in the root directory of that repository. An example workflow is to generate the index.html from Quarto and then copy that in.\nPersonally, I use a script in the scripts dir that copies the generated Quarto directory to the public directory in “scripts/publish_ee_book_to_github_io_site”"
  },
  {
    "objectID": "Appendices/hosting_notes.html#pasting-images",
    "href": "Appendices/hosting_notes.html#pasting-images",
    "title": "21  Hosting notes",
    "section": "21.2 Pasting images",
    "text": "21.2 Pasting images\nNOTE: This is easiest to do in ipynb NOT in qmd because of the built-in options for pasting images into notebooks.\nNote that there are two methods: paste (ctrl-v) directly into a markdown cell of a ipynb. This adds it as an “attachment” where the raw binary image code is written into text in the cell’s attribute (and thus there is no external file saved). The other is the use the Paste Image VS Code plug in, which lets you actually write a file in a gnerated location for the PNG. This is called by ctrl-alt-v\nOption 1\n\n\n\nimage.png\n\n\nOption 2"
  },
  {
    "objectID": "Appendices/quarto_notes.html",
    "href": "Appendices/quarto_notes.html",
    "title": "22  Cross References",
    "section": "",
    "text": "For tables produced by executable code cells, include a label with a tbl- prefix to make them cross-referenceable. For example:\n\nfrom IPython.display import Markdown\n# from tabulate import tabulate\n# table = [[\"Sun\",696000,1989100000],\n#          [\"Earth\",6371,5973.6],\n#          [\"Moon\",1737,73.5],\n#          [\"Mars\",3390,641.85]]\n# Markdown(tabulate(\n#   table, \n#   headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n# ))\n\n\n?(caption)"
  }
]
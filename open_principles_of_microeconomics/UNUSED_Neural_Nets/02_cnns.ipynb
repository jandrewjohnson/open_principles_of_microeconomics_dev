{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Automatic feature selection with LASSO regression\n",
    "\n",
    "In this notebook we will learn how LASSO (Least Absolute Shrinkage and Selection Operator) regression works and how it can assist in automatically selecting which variables should be included using a **Cross-Validation** perspective.\n",
    "\n",
    "#### Start by importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets\n",
    "#mnist = tensorflow_datasets.load('mnist')\n",
    "\n",
    "def run_cnn():\n",
    "   mnist = tensorflow_datasets.load('mnist')\n",
    "   learning_rate = 0.0001\n",
    "   epochs = 10\n",
    "   batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtime\u001b[39;00m \u001b[39mimport\u001b[39;00m time\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import math\n",
    " \n",
    " \n",
    "from include.data import get_data_set\n",
    "from include.model import model, lr\n",
    " \n",
    " \n",
    "train_x, train_y = get_data_set(\"train\")\n",
    "test_x, test_y = get_data_set(\"test\")\n",
    "tf.set_random_seed(21)\n",
    "x, y, output, y_pred_cls, global_step, learning_rate = model()\n",
    "global_accuracy = 0\n",
    "epoch_start = 0\n",
    " \n",
    " \n",
    "# PARAMS\n",
    "_BATCH_SIZE = 128\n",
    "_EPOCH = 60\n",
    "_SAVE_PATH = \"./tensorboard/cifar-10-v1.0.0/\"\n",
    " \n",
    " \n",
    "# LOSS AND OPTIMIZER\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999,\n",
    "                                   epsilon=1e-08).minimize(loss, global_step=global_step)\n",
    " \n",
    " \n",
    "# PREDICTION AND ACCURACY CALCULATION\n",
    "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    " \n",
    " \n",
    "# SAVER\n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "train_writer = tf.summary.FileWriter(_SAVE_PATH, sess.graph)\n",
    " \n",
    " \n",
    "try:\n",
    "    print(\"Trying to restore last checkpoint ...\")\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
    "    saver.restore(sess, save_path=last_chk_path)\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except ValueError:\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    " \n",
    " \n",
    "def train(epoch):\n",
    "    global epoch_start\n",
    "    epoch_start = time()\n",
    "    batch_size = int(math.ceil(len(train_x) / _BATCH_SIZE))\n",
    "    i_global = 0\n",
    " \n",
    "    for s in range(batch_size):\n",
    "        batch_xs = train_x[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
    "        batch_ys = train_y[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
    " \n",
    "        start_time = time()\n",
    "        i_global, _, batch_loss, batch_acc = sess.run(\n",
    "            [global_step, optimizer, loss, accuracy],\n",
    "            feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)})\n",
    "        duration = time() - start_time\n",
    " \n",
    "        if s % 10 == 0:\n",
    "            percentage = int(round((s/batch_size)*100))\n",
    " \n",
    "            bar_len = 29\n",
    "            filled_len = int((bar_len*int(percentage))/100)\n",
    "            bar = '=' * filled_len + '>' + '-' * (bar_len - filled_len)\n",
    " \n",
    "            msg = \"Global step: {:>5} - [{}] {:>3}% - acc: {:.4f} - loss: {:.4f} - {:.1f} sample/sec\"\n",
    "            print(msg.format(i_global, bar, percentage, batch_acc, batch_loss, _BATCH_SIZE / duration))\n",
    " \n",
    "    test_and_save(i_global, epoch)\n",
    " \n",
    " \n",
    "def test_and_save(_global_step, epoch):\n",
    "    global global_accuracy\n",
    "    global epoch_start\n",
    " \n",
    "    i = 0\n",
    "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
    "    while i < len(test_x): \n",
    "        j = min(i + _BATCH_SIZE, len(test_x)) \n",
    "        batch_xs = test_x[i:j, :] \n",
    "        batch_ys = test_y[i:j, :] \n",
    "        predicted_class[i:j] = sess.run( y_pred_cls, feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)} ) \n",
    "        i = j \n",
    "        correct = (np.argmax(test_y, axis=1) == predicted_class) \n",
    "        acc = correct.mean()*100 \n",
    "        correct_numbers = correct.sum() \n",
    "        hours, rem = divmod(time() - epoch_start, 3600) \n",
    "        minutes, seconds = divmod(rem, 60) \n",
    "        mes = \" Epoch {} - accuracy: {:.2f}% ({}/{}) - time: {:0>2}:{:0>2}:{:05.2f}\"\n",
    "        print(mes.format((epoch+1), acc, correct_numbers, len(test_x), int(hours), int(minutes), seconds))\n",
    " \n",
    "    if global_accuracy != 0 and global_accuracy < acc: \n",
    "        summary = tf.Summary(value=[ tf.Summary.Value(tag=\"Accuracy/test\", simple_value=acc), ])\n",
    "        train_writer.add_summary(summary, _global_step) \n",
    "        saver.save(sess, save_path=_SAVE_PATH, global_step=_global_step) \n",
    "        mes = \"This epoch receive better accuracy: {:.2f} > {:.2f}. Saving session...\"\n",
    "        print(mes.format(acc, global_accuracy))\n",
    "        global_accuracy = acc\n",
    " \n",
    "    elif global_accuracy == 0:\n",
    "        global_accuracy = acc\n",
    " \n",
    "    print(\"###########################################################################################################\")\n",
    " \n",
    " \n",
    "def main():\n",
    "    train_start = time()\n",
    " \n",
    "    for i in range(_EPOCH):\n",
    "        print(\"Epoch: {}/{}\".format((i+1), _EPOCH))\n",
    "        train(i)\n",
    " \n",
    "    hours, rem = divmod(time() - train_start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    mes = \"Best accuracy pre session: {:.2f}, time: {:0>2}:{:0>2}:{:05.2f}\"\n",
    "    print(mes.format(global_accuracy, int(hours), int(minutes), seconds))\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    " \n",
    " \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    " \n",
    "from include.data import get_data_set\n",
    "from include.model import model\n",
    " \n",
    " \n",
    "test_x, test_y = get_data_set(\"test\")\n",
    "x, y, output, y_pred_cls, global_step, learning_rate = model()\n",
    " \n",
    " \n",
    "_BATCH_SIZE = 128\n",
    "_CLASS_SIZE = 10\n",
    "_SAVE_PATH = \"./tensorboard/cifar-10-v1.0.0/\"\n",
    " \n",
    " \n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    " \n",
    " \n",
    "try:\n",
    "    print(\"\n",
    "Trying to restore last checkpoint ...\")\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
    "    saver.restore(sess, save_path=last_chk_path)\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except ValueError:\n",
    "    print(\"\n",
    "Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    " \n",
    " \n",
    "def main():\n",
    "    i = 0\n",
    "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
    "    while i < len(test_x):\n",
    "        j = min(i + _BATCH_SIZE, len(test_x))\n",
    "        batch_xs = test_x[i:j, :]\n",
    "        batch_ys = test_y[i:j, :]\n",
    "        predicted_class[i:j] = sess.run(y_pred_cls, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        i = j\n",
    " \n",
    "    correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
    "    acc = correct.mean() * 100\n",
    "    correct_numbers = correct.sum()\n",
    "    print()\n",
    "    print(\"Accuracy on Test-Set: {0:.2f}% ({1} / {2})\".format(acc, correct_numbers, len(test_x)))\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    " \n",
    " \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# This activity compares different values for regularization parameter ‘alpha’.\n",
    "# The plot shows that different alphas yield different decision functions.\n",
    "\n",
    "# Recall that alpha is a parameter for regularization term, aka penalty term, that combats\n",
    "# overfitting by constraining the size of the weights. Increasing alpha may fix\n",
    "# high variance (a sign of overfitting) by encouraging smaller weights,\n",
    "# resulting in a decision boundary plot that appears with lesser curvatures.\n",
    "# Similarly, decreasing alpha may fix high bias (a sign of underfitting) by\n",
    "# encouraging larger weights, potentially resulting in a more complicated decision boundary.\n",
    "\n",
    "# Create a vector of alphas to test.\n",
    "alphas = np.logspace(-5, 3, 5)\n",
    "\n",
    "# Assign those alphas to some names.\n",
    "# Notice also a very cool feature in python called \"list comprehension\":\n",
    "\n",
    "# Instead of\n",
    "# for i in alphas:\n",
    "#   names.append(alpha ' + str(i))\n",
    "\n",
    "# List comprehension defines the for loop INSIDE a list.\n",
    "names = ['alpha ' + str(i) for i in alphas]\n",
    "\n",
    "# print('names', names)\n",
    "\n",
    "# Now for the heavy lifting\n",
    "# We will create a Pipeline of transforms with a final estimator.\n",
    "\n",
    "# Sequentially apply a list of transforms and a final estimator.\n",
    "# Intermediate steps of the pipeline must be ‘transforms’, that is,\n",
    "# they must implement fit and transform methods. The final estimator only\n",
    "# needs to implement fit.\n",
    "\n",
    "# The purpose of the pipeline is to assemble several steps that can be cross-validated\n",
    "# together while setting different parameters.\n",
    "\n",
    "classifiers = []\n",
    "for i in alphas:\n",
    "\n",
    "    # Assign a classifier into the pipeline along with a scaler object.\n",
    "    classifiers.append(make_pipeline(\n",
    "                       StandardScaler(),\n",
    "                       MLPClassifier(solver='lbfgs', alpha=i,\n",
    "                                     random_state=1, max_iter=2000,\n",
    "                                     early_stopping=True,\n",
    "                                     hidden_layer_sizes=[100, 100])\n",
    "                       ))\n",
    "\n",
    "# Use one of sklearn's built-in data generators to generate some 2 dimensional (2 feature)\n",
    "# Data to classify.\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=0, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "# this generates the following three datasets that each pose unique challenges for classification\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable]\n",
    "\n",
    "\n",
    "# Take a look at the results to see different classifications, along with how they scale with\n",
    "# the regularization parameter.\n",
    "figure = plt.figure(figsize=(17, 9))\n",
    "i = 1\n",
    "\n",
    "h = .02  # step size in the mesh (the thing we'll actually plot).\n",
    "# iterate over datasets\n",
    "for X, y in datasets:\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='black', s=25)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6, edgecolors='black', s=25)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('8222env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0db313e0ad7b6749a6d098fb61fddaded88cbd823278030b75fa0893942c8f77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Intro\n",
    "\n",
    "\n",
    "\n",
    "# Remember to pull the latest code!\n",
    "\n",
    "\n",
    "\n",
    "Reminder: HW4 due by midnight tonight.\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1240.png)\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1241.png)\n",
    "\n",
    "\n",
    "\n",
    "Fun trick of the day: You can use Markdown in Google Docs\\! \\(have to enable it first\\)\n",
    "\n",
    "\n",
    "\n",
    "# Agenda\n",
    "\n",
    "\n",
    "\n",
    "* Comment on our hilariously \"effective\" p\\-hacking routine from last lecture\\.\n",
    "\n",
    "  * We'll address how can we do this in a legitimate way\\.\n",
    "\n",
    "* Introduce regularization and shrinkage\n",
    "\n",
    "  * Discuss why we should do it\\.\n",
    "\n",
    "* Learn Ridge and LASSO ML algorithms\n",
    "\n",
    "* Apply them in Code\n",
    "\n",
    "\n",
    "\n",
    "# Econometrics versus CV\n",
    "\n",
    "\n",
    "\n",
    "* Econometrics: X\\, Y not split into testing and training\\.\n",
    "\n",
    "  * Y\\_hat is generated from OLS operating on All Data\\.\n",
    "\n",
    "* In the linear\\_regression notebook\\, we used Sklearn to automatically solve for the best set of \\(two\\) coefficients\\.\n",
    "\n",
    "  * In this\\, we took our first baby step to CV by splitting into Training and Testing\\.\n",
    "\n",
    "  * We learned SVM applied to this training/testing paradigm\\.\n",
    "\n",
    "* This is still not \"Real CV\" because we used the Test Data in finding the best set of coefficients\\.\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1242.png)\n",
    "\n",
    "\n",
    "\n",
    "# Real CV\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1243.png)\n",
    "\n",
    "\n",
    "\n",
    "* Therefor\\, cross\\-validation creates a second split of the training data\\.\n",
    "\n",
    "  * Iteratively solves on this second\\-split\\.\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1244.png)\n",
    "\n",
    "\n",
    "\n",
    "# But that process can be extrapolated up to better utilize our (possibly scarce) data\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1245.png)\n",
    "\n",
    "\n",
    "\n",
    "* Keeping track of splits and folds is hard\n",
    "\n",
    "  * We're going to use GridSearchCV to do it all for us\\.\n",
    "\n",
    "\n",
    "\n",
    "# Reminder on the bias-variance tradeoff\n",
    "\n",
    "\n",
    "\n",
    "* Reminder: ML lingo uses the word \"Variance\" in a slightly different way than we're used to:\n",
    "\n",
    "  * Here\\, variance measures the difference in performance between on the training vs testing data\n",
    "\n",
    "    * This is almost unrelated to the more familiar \\(to us\\) use of variance to describe a distribution\\.\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1246.png)\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1247.png)\n",
    "\n",
    "\n",
    "\n",
    "# One way to reduce variance: Shrinkage\n",
    "\n",
    "\n",
    "\n",
    "* Shrinkage estimators offer lower variance by construction\n",
    "\n",
    "  * It's literally just lowering your beta values\\.\n",
    "\n",
    "  * The more we shrink coefficient estimates\\, the lower their variance across samples\\.\n",
    "\n",
    "* To build intuition on this\\, start with the extreme case: shrink all slope estimates to zero\\, make constant predictions\\.\n",
    "\n",
    "  * Performance would be constant\\!\n",
    "\n",
    "\n",
    "\n",
    "# Quick example and implications for bias\n",
    "\n",
    "\n",
    "\n",
    "# Starting with OLS\n",
    "\n",
    "\n",
    "\n",
    "* When we have many observations\\, OLS does a good job\n",
    "\n",
    "  * Why? Because with tons of observations\\, it is more likely the training data reflect other possible data\\.\n",
    "\n",
    "    * E\\.g\\.\\, suppose we have measurements of weight and size of mice\\. Fits pretty well\\.\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1248.png)\n",
    "\n",
    "\n",
    "\n",
    "# But what if we have fewer data?\n",
    "\n",
    "\n",
    "\n",
    "* Suppose we only get to fit the data on a small subset \\(red\\)\n",
    "\n",
    "  * The resulting estimate is not similar to the full data in green\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1249.png)\n",
    "\n",
    "\n",
    "\n",
    "# Variance on out-of-sample observations is huge\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1250.png)\n",
    "\n",
    "\n",
    "\n",
    "* Our predicted line will have minimum bias on the training data\n",
    "\n",
    "  * It's the \"Best Linear Unbiased Estimate\" \\(BLUE\\)\n",
    "\n",
    "* But\\, there is huge variance on green\\.\n",
    "\n",
    "\n",
    "\n",
    "* Main idea of regularization:\n",
    "\n",
    "  * What if we shrink the COEFFICIENT value in the OLS regression\n",
    "\n",
    "  * This will obviously introduce more bias\n",
    "\n",
    "* But\\, as we will see\\, this almost always reduces out\\-of\\-sample variance\n",
    "\n",
    "  * This is also referred to as \"shrinkage\"\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1251.png)\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1252.png)\n",
    "\n",
    "\n",
    "\n",
    "* __Why did we just do this?__\n",
    "\n",
    "* In general \\(VERY VERY OFTEN\\) shrinking our coefficients will improve out\\-of\\-sample performance\n",
    "\n",
    "  * It is so important that in big data and ML applications\\, almost everything is regularized in some way\\.\n",
    "\n",
    "\n",
    "\n",
    "# Shrinkage estimators: general form\n",
    "\n",
    "\n",
    "\n",
    "# Shrinkage estimators: Lagrangian form\n",
    "\n",
    "\n",
    "\n",
    "# Ridge regression\n",
    "\n",
    "\n",
    "\n",
    "# Ridge Regression intuition\n",
    "\n",
    "\n",
    "\n",
    "# Solution to Ridge\n",
    "\n",
    "\n",
    "\n",
    "# Note about coefficient scaling\n",
    "\n",
    "\n",
    "\n",
    "# LASSO: Least Absolute Shrinkage and Selection Operator\n",
    "\n",
    "\n",
    "\n",
    "![](img\\\\1253.png)\n",
    "\n",
    "\n",
    "\n",
    "# LASSO: Not just another penalty.\n",
    "\n",
    "\n",
    "\n",
    "# LASSO and variable selection\n",
    "\n",
    "\n",
    "\n",
    "* When solved\\, LASSO may result in estimates of exactly zero for some parameters\\.\n",
    "\n",
    "  * In other words\\, LASSO shrinks parameter estimates\\, and shrinks some to zero\n",
    "\n",
    "  * __This results in automated variable selection__ \\.\n",
    "\n",
    "* Switch to VS Code\\, notebook lectures/03\\_Regulariation/01\\_ridge\\_and\\_lasso\\.ipynb\n",
    "\n",
    "\n",
    "\n",
    "# So what did we just do?\n",
    "\n",
    "\n",
    "\n",
    "# Solution: reduce bias by using OLS after\n",
    "\n",
    "\n",
    "\n",
    "* \"post\\-LASSO regression\"\n",
    "\n",
    "  * Use LASSO to estimate the main model\\, letting it do variable selection\n",
    "\n",
    "  * Estimate the main model with selected variables from step 1\\. This time use OLS\\.\n",
    "\n",
    "* Switch back to our notebook where we will implement this\\.\n",
    "\n",
    "\n",
    "\n",
    "# Appendix\n",
    "\n",
    "\n",
    "\n",
    "# Elastic Net: LASSO with a ridge\n",
    "\n",
    "\n",
    "\n",
    "# IV and LASSO\n",
    "\n",
    "\n",
    "\n",
    "# Motivation\n",
    "\n",
    "\n",
    "\n",
    "# Many possible instruments in big data contexts\n",
    "\n",
    "\n",
    "\n",
    "# How many instruments?\n",
    "\n",
    "\n",
    "\n",
    "* Asymptotic theory:\n",
    "\n",
    "  * IF all instruments are exogenous and relevant\\, then they contain useful information\\.\n",
    "\n",
    "  * For the most efficient \\(low variance\\) estimator\\, we should use all available information\\. Question is just how to combine it\\.\n",
    "\n",
    "  * 2SLS can help recover optimal combination of those instruments\\.\n",
    "\n",
    "* But\\, we often have finite samples\n",
    "\n",
    "* And\\, having too many instruments can introduce bias\\!\n",
    "\n",
    "  * Hence\\, we need some well\\-thought\\-out model selection approach\\.\n",
    "\n",
    "\n",
    "\n",
    "# Correct for overfitting-bias with LASSO\n",
    "\n",
    "\n",
    "\n",
    "* If bias arises from overfitting the first stage\\, we could try to limit overfitting\\.\n",
    "\n",
    "  * Belloni\\, Chen\\, Chernozhukov\\, Hansen \\(2012 Econometrica\\): LASSO\\!\n",
    "\n",
    "* Rough idea:\n",
    "\n",
    "  * Estimate first stage via LASSO \\(with specific penalization scheme\\)\n",
    "\n",
    "  * Use instruments selected via LASSO in first stage estimated via OLS\n",
    "\n",
    "  * Use fitted values in second stage\n",
    "\n",
    "\n",
    "\n",
    "# Many instruments, many controls\n",
    "\n",
    "\n",
    "\n",
    "# IV and not LASSO\n",
    "\n",
    "\n",
    "\n",
    "The world isn't always sparse\n",
    "\n",
    "\n",
    "\n",
    "# Other options for the first stage\n",
    "\n",
    "\n",
    "\n",
    "# Deep IV: setup\n",
    "\n",
    "\n",
    "\n",
    "# Deep IV: idea\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('8222env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0db313e0ad7b6749a6d098fb61fddaded88cbd823278030b75fa0893942c8f77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

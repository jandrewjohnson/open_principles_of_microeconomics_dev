{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Assignment 2: Using post-LASSO on large spatial data\n",
    "\n",
    "This assignment will give you a real (active) research topic that I've discussed a little bit in class: predicting carbon storage as a function of high-resolution gridded data. In the class google drive you will find all the data you need. I added it just recently so if you don't have it, be sure to go get it first.\n",
    "\n",
    "This assignment will have you use the automated variable selection approach within LASSO to deal with a common situation in regressions on raster-stacks: we have so much data everything is significant but will lead to massive overfitting. The basic approach used here will involve reading in 2d rasters, flattening them into a 1d column ready to add to a dataframe shaped object, which we will use as our X matrix.\n",
    "\n",
    "Please turn in the completed Notebook (.ipynb) file that includes the results you generate. \n",
    "\n",
    "Below is some starter code along with specific assignment questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from osgeo import gdal\n",
    "from sklearn.linear_model import Lasso\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Download the data and set paths\n",
    "\n",
    "Download the data and assign a relative path to the soyo_tile directory in that assignment directory. Here, I actually want you to use the exact path below so that it works on my machine too. It is your task to ensure your script runs in the right location and the data is stored in the right location that this relative path works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = '../../Data/assignment_5/soyo_tile'\n",
    "\n",
    "\n",
    "# Step 2: assign each of the raster paths in the directory to a dictionary for later use. I've included\n",
    "# most of the code (so you don't have to waste your time typing), but add in the three missing\n",
    "# paths.\n",
    "\n",
    "raster_paths = {}\n",
    "raster_paths['agb_observed_baccini_2000_30m'] = os.path.join(data_dir, \"agb_observed_baccini_2000_30m.tif\")\n",
    "\n",
    "raster_paths['CRFVOL_M_sl1_250m'] = os.path.join(data_dir, \"CRFVOL_M_sl1_250m.tif\")\n",
    "raster_paths['HISTPR_250m'] = os.path.join(data_dir, \"HISTPR_250m.tif\")\n",
    "raster_paths['OCDENS_M_sl1_250m'] = os.path.join(data_dir, \"OCDENS_M_sl1_250m.tif\")\n",
    "raster_paths['PHIHOX_M_sl1_250m'] = os.path.join(data_dir, \"PHIHOX_M_sl1_250m.tif\")\n",
    "raster_paths['roughness_30s'] = os.path.join(data_dir, \"roughness_30s.tif\")\n",
    "raster_paths['SLGWRB_250m'] = os.path.join(data_dir, \"SLGWRB_250m.tif\")\n",
    "raster_paths['SLTPPT_M_sl1_250m'] = os.path.join(data_dir, \"SLTPPT_M_sl1_250m.tif\")\n",
    "raster_paths['SNDPPT_M_sl1_250m'] = os.path.join(data_dir, \"SNDPPT_M_sl1_250m.tif\")\n",
    "raster_paths['terrain_ruggedness_index_30s'] = os.path.join(data_dir, \"terrain_ruggedness_index_30s.tif\")\n",
    "raster_paths['TEXMHT_M_sl1_250m'] = os.path.join(data_dir, \"TEXMHT_M_sl1_250m.tif\")\n",
    "raster_paths['wc2.0_bio_30s_01'] = os.path.join(data_dir, \"wc2.0_bio_30s_01.tif\")\n",
    "raster_paths['alt_30s'] = os.path.join(data_dir, \"alt_30s.tif\")\n",
    "raster_paths['AWCh1_M_sl1_250m'] = os.path.join(data_dir, \"AWCh1_M_sl1_250m.tif\")\n",
    "raster_paths['BDRICM_M_250m'] = os.path.join(data_dir, \"BDRICM_M_250m.tif\")\n",
    "raster_paths['BDRLOG_M_250m'] = os.path.join(data_dir, \"BDRLOG_M_250m.tif\")\n",
    "raster_paths['BLDFIE_M_sl1_250m'] = os.path.join(data_dir, \"BLDFIE_M_sl1_250m.tif\")\n",
    "\n",
    "\n",
    "# Step 3: Our dependent variable will be 30 meter observations of carbon storage from\n",
    "# Baccini et al. (unpublished, but soon to be published) data. The label I assigned in the dictionary\n",
    "# above was agb_observed_baccini_2000_30m for this variable.\n",
    "# Use gdal.Open, GetRasterBand(1) and ReadAsArray() to read this geotiff as a numpy file\n",
    "# Side note:\n",
    "# If you get an error like: \"ERROR 4: This is a BigTIFF file.  BigTIFF is not supported by this version of GDAL and libtiff.\"\n",
    "# make sure you have conda installed gdal from the CONDA FORGE  using the command \"conda install gdal -c conda-forge\" option.\n",
    "\n",
    "\n",
    "# Step 4, Create an empty numpy array (or full of zeros) of the right shape to house all our raster data.\n",
    "# A very CPU-efficient way of arranging a stack of 2d rasters (which would be 3d once stacked up), is\n",
    "# to flatten each 2d raster into a longer 1d array. This will go into our X matrix.\n",
    "# In order to create the right sized X matrix, first get the n_obs and n_vars by inspecting\n",
    "# the dependent variable raster and the dictionary of inputs above.\n",
    "# Note that the n_vars should be the number of INDEPENDENT and DEPENDENT variables\n",
    "# report in your WORD DOCUMENT the size of the data_array you created.\n",
    "\n",
    "\n",
    "# Step 5, Iterate through the dictionary and load each raster as a 2d array, flatten it to 1d\n",
    "# using the .flatten() method in numpy. Assign this 1d array to the correct column\n",
    "# of the data array. By convention, the depvar will be the first column.\n",
    "\n",
    "# Hint, assuming you have arranged your X array in the correct way, it should have observations (pixels)\n",
    "# as rows and variables as cols. Given that each flattened array is for one variable and is as long as there\n",
    "# are rows, a convenient way of assigning it would be to use numpy slice notation, potentially similar to:\n",
    "# data_array[:, column_index_integer].\n",
    "# The first colon just denotes the whole row and the column index is an integer you could create pointing to the right row.\n",
    "# Some incomplete code to get you started is below.\n",
    "\n",
    "for name, path in raster_paths.items():\n",
    "    print('Loading', path)\n",
    "    flattened_raster_array = band.ReadAsArray().flatten()\n",
    "    data_array[:, col_index] = flattened_raster_array\n",
    "    feature_names.append(name)\n",
    "\n",
    "\n",
    "# Step 6, extract the first array row of the data_array and assign it to y. Assign the rest to X.\n",
    "\n",
    "\n",
    "# Step 7, split the X and y into testing and training data such that\n",
    "# the training data is the first million pixels and the testing data is the next 200,000\n",
    "# Do this using numpy slice notation on the X and y variables you created.\n",
    "\n",
    "\n",
    "# Step 8 (optional but useful). To make the code run faster, we are going to\n",
    "# use every 10th pixel. We can easily get this via numpy slicing again, using\n",
    "# x_train[::10] to get every 10th pixel.\n",
    "\n",
    "\n",
    "# Step 9, create a Lasso object (using the default penalty term alpha)\n",
    "# and fit it to the training data. Create\n",
    "# and print out a vector of predicted carbon values. Also print out the score\n",
    "# using the lasso object's .score() method on the TESTING data.\n",
    "# Add the fitted lasso score to your WORD DOCUMENT.\n",
    "\n",
    "\n",
    "# Step 10, optional and just for fun. To view how our projections LOOK, we can\n",
    "# create a predicted matrix on the whole X, reshape it back into the\n",
    "# original 2d shape and look at it. You can compare this to the input array\n",
    "# to visualize how it looks. Note that this will only work if you name your objects\n",
    "# like mine.\n",
    "\n",
    "# full_prediction = fitted_lasso.predict(X)\n",
    "# prediction_2d = full_prediction.reshape(2000, 2000)\n",
    "# plt.imshow(prediction_2d)\n",
    "# plt.show()\n",
    "#\n",
    "# plt.imshow(array)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Step 11, Create a list of 30 alphas using np.logspace(-1, 3, 30). Using a for loop\n",
    "# iterate over those alphas and run the Lasso model like above, but using the alpha values\n",
    "# in the loop. Print out the fit score at each step. Using matplotlib, plot how this\n",
    "# value changes as alpha changes. Finally, extract the best alpha of the bunch.\n",
    "# Put the plot of alphas and their scores in your WORD DOCUMENT along with the value\n",
    "# of the optimal alpha.\n",
    "\n",
    "alphas = np.logspace(-1, 3, 30)\n",
    "\n",
    "\n",
    "# Step 12, rerun the lasso with that best value and identify all of the coefficiencts\n",
    "# that were \"selected\" ie had non-zero values. Save these coefficient indices and labels\n",
    "# to a list.\n",
    "\n",
    "\n",
    "# Step 13, Using Statsmodels, run an OLS version on the selected variables.\n",
    "# Copy and paste your resulting OLS.summary() table to your WORD DOCUMENT. In addition\n",
    "# add to your WORD DOCUMENT a description of how this result is better than a vanilla OLS.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.6 ('8222env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0db313e0ad7b6749a6d098fb61fddaded88cbd823278030b75fa0893942c8f77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
